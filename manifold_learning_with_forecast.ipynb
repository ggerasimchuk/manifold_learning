{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Behavior Manifold & Forecast\n",
    "\n",
    "Обновлённый ноутбук объединяет основные шаги пайплайна PBM в единое рабочее место:\n",
    "1. Загрузка и очистка сырых ежемесячных отчётов.\n",
    "2. Предобработка профилей (Step 1) и сохранение результатов.\n",
    "3. Построение дескрипторов, manifold-embedding и кластеризация.\n",
    "4. Генерация отчётов по сегментации.\n",
    "5. Построение прогнозов по префиксу (20 → 100).\n",
    "\n",
    "Ниже каждая секция изолирована функциями и настройками, что упрощает повторный запуск и адаптацию под другие проекты.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Импорт, конфигурация и утилиты\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"SCIPY_ARRAY_API\"] = \"1\"\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.width = 120\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Базовые пути и параметры выполнения\n",
    "DATA_DIR = Path(\"data/wells\")\n",
    "FORECAST_EXPORT_DIR = Path(\"reports/forecast_exports\")\n",
    "PBM_REPORT_DIR = Path(\"reports/pbm_report_exports\")\n",
    "\n",
    "# Ограничители для быстрых прогонов (None -> использовать всё)\n",
    "MAX_CSV_FILES: Optional[int] = None   # например, 5 чтобы взять только первые файлы\n",
    "SAMPLE_WELLS: Optional[int] = None    # например, 500 чтобы ограничить число скважин\n",
    "\n",
    "# Параметры шагов\n",
    "RUN_MANIFOLD = True\n",
    "RUN_CLUSTERING = True\n",
    "RUN_PBM_REPORT = False  # отчёт требует matplotlib/hdbscan и может быть тяжёлым\n",
    "RUN_FORECAST = True\n",
    "\n",
    "FORECAST_PREFIX = 20\n",
    "RANDOM_SEED = 59\n",
    "\n",
    "pipeline_state: Dict[str, Any] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def discover_csv_files(data_dir: Path, limit: Optional[int] = None) -> list[Path]:\n",
    "    \"\"\"Поиск CSV с исходными данными.\"\"\"\n",
    "    if not data_dir.exists():\n",
    "        raise FileNotFoundError(f\"Каталог {data_dir} не найден.\")\n",
    "    csv_files = sorted(data_dir.glob(\"*.csv\"))\n",
    "    if limit is not None:\n",
    "        csv_files = csv_files[:limit]\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"В каталоге {data_dir} нет csv-файлов.\")\n",
    "    return csv_files\n",
    "\n",
    "RENAME_MAP = {\n",
    "    \"BBLS_OIL_COND\": \"oil\",\n",
    "    \"MCF_GAS\": \"gas\",\n",
    "    \"BBLS_WTR\": \"water\",\n",
    "    \"API_WellNo\": \"well_name\",\n",
    "    \"RptDate\": \"date\",\n",
    "    \"DAYS_PROD\": \"days_prod\",\n",
    "}\n",
    "\n",
    "def load_raw_well_data(data_dir: Path, limit_files: Optional[int] = None) -> pd.DataFrame:\n",
    "    csv_files = discover_csv_files(data_dir, limit_files)\n",
    "    frames = [pd.read_csv(path) for path in csv_files]\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    print(f\"Считано файлов: {len(csv_files)} — строк: {len(df):,}\")\n",
    "    return df\n",
    "\n",
    "def clean_well_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.rename(columns=RENAME_MAP)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    drop_cols = [c for c in (\"Lease_Unit\", \"Formation\") if c in df.columns]\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols)\n",
    "    df = df[(df[\"oil\"] >= 0) & (df[\"gas\"] >= 0) & (df[\"water\"] >= 0)].copy()\n",
    "    df = df.sort_values(by=[\"well_name\", \"date\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def subset_wells(df: pd.DataFrame, n_wells: Optional[int] = None) -> pd.DataFrame:\n",
    "    if n_wells is None:\n",
    "        return df\n",
    "    wells = (\n",
    "        df[\"well_name\"]\n",
    "        .dropna()\n",
    "        .drop_duplicates()\n",
    "        .sort_values()\n",
    "        .head(n_wells)\n",
    "    )\n",
    "    return df[df[\"well_name\"].isin(wells)].copy()\n",
    "\n",
    "def prepare_dataset(data_dir: Path, limit_files: Optional[int] = None, sample_wells: Optional[int] = None) -> pd.DataFrame:\n",
    "    raw = load_raw_well_data(data_dir, limit_files)\n",
    "    cleaned = clean_well_data(raw)\n",
    "    subset = subset_wells(cleaned, sample_wells)\n",
    "    print(f\"Итоговый датафрейм: {subset.shape[0]:,} строк, {subset['well_name'].nunique()} скважин\")\n",
    "    return subset\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    df = prepare_dataset(DATA_DIR, MAX_CSV_FILES, SAMPLE_WELLS)\n",
    "except FileNotFoundError as exc:\n",
    "    print(f\"⚠️ {exc}\")\n",
    "    df = None\n",
    "else:\n",
    "    pipeline_state[\"dataframe\"] = df\n",
    "    display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Предобработка профилей (PBM Step 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tools.preprocessing import PreprocConfig, preprocess_profiles\n",
    "\n",
    "def run_preprocessing(df: pd.DataFrame, cfg: Optional[PreprocConfig] = None) -> Dict[str, Any]:\n",
    "    if df is None:\n",
    "        raise ValueError(\"Нет исходных данных — убедитесь, что предыдущая ячейка выполнена успешно.\")\n",
    "    cfg = cfg or PreprocConfig()\n",
    "    print(f\"Запуск preprocess_profiles для {df['well_name'].nunique()} скважин (T={cfg.T})\")\n",
    "    out = preprocess_profiles(df, cfg)\n",
    "    print(\"panel_long shape:\", out[\"panel_long\"].shape)\n",
    "    print(\"tensor shape:\", out[\"X\"].shape)\n",
    "    display(out[\"panel_long\"].head(12))\n",
    "    out[\"config\"] = dict(out.get(\"config\", {}))\n",
    "    return out\n",
    "\n",
    "preproc_cfg = PreprocConfig()\n",
    "preproc_out = run_preprocessing(df, preproc_cfg) if df is not None else None\n",
    "pipeline_state[\"preprocessing\"] = preproc_out\n",
    "out = preproc_out  # совместимость со старыми скриптами\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2–3. Компактные признаки и manifold\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    from tools.feature import compute_side_features, scale_features\n",
    "    from tools.manifold import ManifoldConfig, embed_umap_euclid, embed_umap_fastdtw\n",
    "    MANIFOLD_IMPORT_ERROR = None\n",
    "except Exception as exc:\n",
    "    MANIFOLD_IMPORT_ERROR = exc\n",
    "\n",
    "def run_manifold(out_dict: Dict[str, Any], cfg: Optional[ManifoldConfig] = None, sample_size: Optional[int] = None) -> Dict[str, Any]:\n",
    "    if out_dict is None:\n",
    "        raise ValueError(\"Нет результатов предобработки.\")\n",
    "    if MANIFOLD_IMPORT_ERROR is not None:\n",
    "        raise RuntimeError(f\"Не удалось импортировать инструменты manifold: {MANIFOLD_IMPORT_ERROR}\")\n",
    "    cfg = cfg or ManifoldConfig()\n",
    "    panel_long = out_dict[\"panel_long\"]\n",
    "    X = out_dict[\"X\"]\n",
    "    wells = out_dict[\"wells_used\"]\n",
    "    tensor_channels = out_dict[\"tensor_channels\"]\n",
    "    T = int(out_dict.get(\"config\", {}).get(\"T\", X.shape[1]))\n",
    "\n",
    "    feats = compute_side_features(panel_long, T=T)\n",
    "    feats_scaled, scaler = scale_features(feats)\n",
    "    print(\"Side features shape:\", feats_scaled.shape)\n",
    "\n",
    "    Z_euclid, umap_e = embed_umap_euclid(\n",
    "        X,\n",
    "        tensor_channels=tensor_channels,\n",
    "        channels=cfg.channels,\n",
    "        n_neighbors=cfg.n_neighbors,\n",
    "        min_dist=cfg.min_dist,\n",
    "        n_components=cfg.n_components,\n",
    "        random_state=cfg.random_state,\n",
    "    )\n",
    "    print(\"UMAP (euclid) shape:\", Z_euclid.shape)\n",
    "\n",
    "    Z_dtw, sub_idx, dist_matrix, info = embed_umap_fastdtw(\n",
    "        X,\n",
    "        tensor_channels=tensor_channels,\n",
    "        channels=cfg.channels,\n",
    "        cfg=cfg,\n",
    "        sample_size=sample_size,\n",
    "    )\n",
    "    wells_sub = np.array(wells)[sub_idx]\n",
    "    embedding_df = pd.DataFrame({\n",
    "        \"well_name\": wells_sub,\n",
    "        \"x\": Z_dtw[:, 0],\n",
    "        \"y\": Z_dtw[:, 1],\n",
    "    })\n",
    "    display(embedding_df.head())\n",
    "    return {\n",
    "        \"cfg\": cfg,\n",
    "        \"features\": feats,\n",
    "        \"features_scaled\": feats_scaled,\n",
    "        \"scaler\": scaler,\n",
    "        \"Z_euclid\": Z_euclid,\n",
    "        \"Z_dtw\": Z_dtw,\n",
    "        \"sub_idx\": sub_idx,\n",
    "        \"dist_matrix\": dist_matrix,\n",
    "        \"info\": info,\n",
    "        \"embedding_df\": embedding_df,\n",
    "    }\n",
    "\n",
    "manifold_cfg = None\n",
    "manifold_out = None\n",
    "if RUN_MANIFOLD and preproc_out is not None:\n",
    "    manifold_cfg = ManifoldConfig(channels=(\"r_oil_norm\", \"wc\"), random_state=RANDOM_SEED)\n",
    "    manifold_out = run_manifold(preproc_out, manifold_cfg)\n",
    "pipeline_state[\"manifold\"] = manifold_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Кластеризация и поиск аномалий\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    from tools.clustering import (\n",
    "        ClusterConfig,\n",
    "        assign_anomaly_scores,\n",
    "        build_cluster_prototypes,\n",
    "        cluster_hdbscan,\n",
    "        summarize_clusters,\n",
    "    )\n",
    "    CLUSTER_IMPORT_ERROR = None\n",
    "except Exception as exc:\n",
    "    CLUSTER_IMPORT_ERROR = exc\n",
    "\n",
    "def run_clustering(out_dict: Dict[str, Any], manifold_dict: Dict[str, Any], cfg: Optional[ClusterConfig] = None) -> Optional[Dict[str, Any]]:\n",
    "    if out_dict is None or manifold_dict is None:\n",
    "        print(\"Кластеризация пропущена — нет данных manifold.\")\n",
    "        return None\n",
    "    if CLUSTER_IMPORT_ERROR is not None:\n",
    "        raise RuntimeError(f\"Не удалось импортировать инструменты кластеризации: {CLUSTER_IMPORT_ERROR}\")\n",
    "    cfg = cfg or ClusterConfig(min_cluster_size=45, min_samples=12)\n",
    "    embedding = manifold_dict.get(\"Z_dtw\")\n",
    "    sub_idx = manifold_dict.get(\"sub_idx\")\n",
    "    if embedding is None or sub_idx is None:\n",
    "        embedding = manifold_dict[\"Z_euclid\"]\n",
    "        sub_idx = np.arange(embedding.shape[0])\n",
    "    wells_sub = np.array(out_dict[\"wells_used\"])[sub_idx]\n",
    "    res = cluster_hdbscan(embedding, wells_sub.tolist(), cfg)\n",
    "    df_map = res[\"df_map\"]\n",
    "    df_map = assign_anomaly_scores(df_map, embedding, res[\"labels\"], lof_k=35)\n",
    "    summary = summarize_clusters(df_map)\n",
    "    display(summary)\n",
    "    prototypes = build_cluster_prototypes(\n",
    "        out_dict[\"panel_long\"],\n",
    "        df_map,\n",
    "        channels=(\"r_oil_s\", \"wc\", \"gor\", \"r_oil_norm\"),\n",
    "        T=int(out_dict[\"config\"][\"T\"]),\n",
    "        method=\"auto\",\n",
    "    )\n",
    "    print(f\"Silhouette={res['silhouette']:.3f}, DBCV={res['dbcv']:.3f}\")\n",
    "    return {\n",
    "        \"cfg\": cfg,\n",
    "        \"result\": res,\n",
    "        \"df_map\": df_map,\n",
    "        \"summary\": summary,\n",
    "        \"prototypes\": prototypes,\n",
    "    }\n",
    "\n",
    "cluster_cfg = None\n",
    "cluster_out = None\n",
    "if RUN_CLUSTERING and manifold_out is not None:\n",
    "    cluster_cfg = ClusterConfig(min_cluster_size=45, min_samples=12)\n",
    "    cluster_out = run_clustering(preproc_out, manifold_out, cluster_cfg)\n",
    "pipeline_state[\"clustering\"] = cluster_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Экспорт отчёта PBM\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    from tools.make_reports import (\n",
    "        build_html_report,\n",
    "        export_csv_summaries,\n",
    "        save_cluster_distribution_plot,\n",
    "        save_cluster_prototype_plots,\n",
    "        save_pbm_map,\n",
    "    )\n",
    "    REPORT_IMPORT_ERROR = None\n",
    "except Exception as exc:\n",
    "    REPORT_IMPORT_ERROR = exc\n",
    "\n",
    "def build_pbm_report(preproc_dict: Dict[str, Any], manifold_dict: Dict[str, Any], cluster_dict: Dict[str, Any], out_dir: Path) -> Dict[str, Any]:\n",
    "    if cluster_dict is None:\n",
    "        print(\"Отчёт пропущен — кластеризация не выполнена.\")\n",
    "        return {}\n",
    "    if REPORT_IMPORT_ERROR is not None:\n",
    "        raise RuntimeError(f\"Не удалось импортировать генераторы отчётов: {REPORT_IMPORT_ERROR}\")\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    embedding = manifold_dict.get(\"Z_dtw\")\n",
    "    if embedding is None:\n",
    "        embedding = manifold_dict[\"Z_euclid\"]\n",
    "    df_map = cluster_dict[\"df_map\"]\n",
    "    map_png = save_pbm_map(embedding, df_map, str(out_dir))\n",
    "    sizes_png = save_cluster_distribution_plot(df_map, str(out_dir))\n",
    "    T = int(preproc_dict[\"config\"][\"T\"])\n",
    "    proto_pngs = save_cluster_prototype_plots(\n",
    "        preproc_dict[\"panel_long\"],\n",
    "        df_map,\n",
    "        cluster_dict[\"prototypes\"],\n",
    "        channels=(\"r_oil_s\", \"wc\", \"gor\", \"r_oil_norm\"),\n",
    "        T=T,\n",
    "        out_dir=str(out_dir),\n",
    "    )\n",
    "    summary = cluster_dict[\"summary\"]\n",
    "    csv_paths = export_csv_summaries(df_map, summary, str(out_dir), top_anoms=50)\n",
    "    report_path = build_html_report(\n",
    "        str(out_dir),\n",
    "        map_png,\n",
    "        sizes_png,\n",
    "        proto_pngs,\n",
    "        df_map,\n",
    "        summary,\n",
    "        title=\"PBM Report\",\n",
    "    )\n",
    "    print(\"Отчёт сохранён:\", report_path)\n",
    "    return {\n",
    "        \"out_dir\": out_dir,\n",
    "        \"map_png\": map_png,\n",
    "        \"sizes_png\": sizes_png,\n",
    "        \"proto_pngs\": proto_pngs,\n",
    "        \"csv_paths\": csv_paths,\n",
    "        \"report_path\": report_path,\n",
    "    }\n",
    "\n",
    "pbm_report = None\n",
    "if RUN_PBM_REPORT and cluster_out is not None:\n",
    "    pbm_report = build_pbm_report(preproc_out, manifold_out, cluster_out, PBM_REPORT_DIR)\n",
    "pipeline_state[\"pbm_report\"] = pbm_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Прогноз профиля (20 → 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from tools.forecast import (\n",
    "    build_prefix_scaled_channel,\n",
    "    evaluate_forecasts,\n",
    "    knn_forecast,\n",
    "    make_matrices,\n",
    "    multioutput_forecast,\n",
    ")\n",
    "\n",
    "def run_forecast_pipeline(out_dict: Dict[str, Any], T_pref: int, output_dir: Path, rng_seed: int = 59) -> Dict[str, Any]:\n",
    "    if out_dict is None:\n",
    "        raise ValueError(\"Нет результатов предобработки.\")\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    panel_long = out_dict[\"panel_long\"].copy()\n",
    "    wells_used = out_dict[\"wells_used\"]\n",
    "    T = int(out_dict[\"config\"][\"T\"])\n",
    "\n",
    "    panel_long = build_prefix_scaled_channel(\n",
    "        panel_long,\n",
    "        wells_used,\n",
    "        T=T,\n",
    "        T_pref=T_pref,\n",
    "        q=0.90,\n",
    "        rate_col=\"r_oil_s\",\n",
    "        out_col=\"r_oil_pref_norm\",\n",
    "    )\n",
    "\n",
    "    X_pref, Y_suffix_true, Y_full = make_matrices(\n",
    "        panel_long,\n",
    "        wells_used,\n",
    "        T=T,\n",
    "        T_pref=T_pref,\n",
    "        channel=\"r_oil_pref_norm\",\n",
    "        target_col=\"r_oil_s\",\n",
    "    )\n",
    "\n",
    "    Y_pred_knn, knn_info = knn_forecast(X_pref, Y_full, T_pref=T_pref, K=15)\n",
    "    Y_pred_lr, lr_info = multioutput_forecast(panel_long, wells_used, T=T, T_pref=T_pref, Y_full=Y_full, random_state=43)\n",
    "\n",
    "    metrics_knn = evaluate_forecasts(Y_suffix_true, Y_pred_knn)\n",
    "    metrics_lr = evaluate_forecasts(Y_suffix_true, Y_pred_lr)\n",
    "\n",
    "    metrics_df = pd.DataFrame([\n",
    "        {\"model\": \"knn\", **metrics_knn},\n",
    "        {\"model\": \"elasticnet\", **metrics_lr},\n",
    "    ])\n",
    "    metrics_path = output_dir / \"metrics.csv\"\n",
    "    metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "    np.save(output_dir / \"Y_suffix_true.npy\", Y_suffix_true)\n",
    "    np.save(output_dir / \"Y_pred_knn.npy\", Y_pred_knn)\n",
    "    np.save(output_dir / \"Y_pred_enet.npy\", Y_pred_lr)\n",
    "\n",
    "    suffix_cols = [f\"m{t}\" for t in range(T_pref + 1, T + 1)]\n",
    "\n",
    "    def save_suffix_csv(arr: np.ndarray, name: str) -> Path:\n",
    "        df_pred = pd.DataFrame(arr, columns=suffix_cols)\n",
    "        df_pred.insert(0, \"well_name\", wells_used)\n",
    "        path = output_dir / f\"pred_{name}.csv\"\n",
    "        df_pred.to_csv(path, index=False)\n",
    "        return path\n",
    "\n",
    "    suffix_paths = {\n",
    "        \"knn\": save_suffix_csv(Y_pred_knn, \"knn\"),\n",
    "        \"elasticnet\": save_suffix_csv(Y_pred_lr, \"elasticnet\"),\n",
    "    }\n",
    "\n",
    "    full_cols = [f\"m{t+1}\" for t in range(T)]\n",
    "\n",
    "    def save_full_csv(arr: np.ndarray, name: str) -> Path:\n",
    "        df_pred = pd.DataFrame(arr, columns=full_cols)\n",
    "        df_pred.insert(0, \"well_name\", wells_used)\n",
    "        path = output_dir / f\"pred_full_{name}.csv\"\n",
    "        df_pred.to_csv(path, index=False)\n",
    "        return path\n",
    "\n",
    "    Y_hat_knn_full = Y_full.copy()\n",
    "    Y_hat_enet_full = Y_full.copy()\n",
    "    Y_hat_knn_full[:, T_pref:T] = Y_pred_knn\n",
    "    Y_hat_enet_full[:, T_pref:T] = Y_pred_lr\n",
    "\n",
    "    full_paths = {\n",
    "        \"knn\": save_full_csv(Y_hat_knn_full, \"knn\"),\n",
    "        \"elasticnet\": save_full_csv(Y_hat_enet_full, \"elasticnet\"),\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    for i, well in enumerate(wells_used):\n",
    "        for t in range(T):\n",
    "            segment = \"observed\" if t < T_pref else \"forecast\"\n",
    "            rows.append({\n",
    "                \"well_name\": well,\n",
    "                \"t\": t,\n",
    "                \"y_true\": float(Y_full[i, t]) if np.isfinite(Y_full[i, t]) else np.nan,\n",
    "                \"y_pred_knn\": float(Y_hat_knn_full[i, t]) if np.isfinite(Y_hat_knn_full[i, t]) else np.nan,\n",
    "                \"y_pred_elasticnet\": float(Y_hat_enet_full[i, t]) if np.isfinite(Y_hat_enet_full[i, t]) else np.nan,\n",
    "                \"segment\": segment,\n",
    "            })\n",
    "    long_df = pd.DataFrame(rows)\n",
    "    long_path = output_dir / \"pred_long.csv\"\n",
    "    long_df.to_csv(long_path, index=False)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def plot_full_example(idx: int, title_prefix: str, pred_full: np.ndarray) -> Path:\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        ax.plot(range(T), Y_full[idx], label=\"true\")\n",
    "        ax.plot(range(T), pred_full[idx], label=\"pred\")\n",
    "        ax.axvline(T_pref - 1, linestyle=\"--\")\n",
    "        ax.set_title(f\"{title_prefix} — {wells_used[idx]}\")\n",
    "        ax.set_xlabel(\"month index\")\n",
    "        ax.set_ylabel(\"oil rate (r_oil_s)\")\n",
    "        ax.legend()\n",
    "        path = output_dir / f\"{title_prefix.replace(' ', '_').lower()}_{idx}_full.png\"\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(path, dpi=140)\n",
    "        plt.close(fig)\n",
    "        return path\n",
    "\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    valid_idx = np.where(np.isfinite(Y_pred_knn).all(axis=1))[0]\n",
    "    example_imgs: list[Path] = []\n",
    "    if valid_idx.size:\n",
    "        chosen = rng.choice(valid_idx, size=min(6, valid_idx.size), replace=False)\n",
    "        for idx in chosen:\n",
    "            example_imgs.append(plot_full_example(idx, \"knn_example\", Y_hat_knn_full))\n",
    "            example_imgs.append(plot_full_example(idx, \"enet_example\", Y_hat_enet_full))\n",
    "\n",
    "    html = f\"\"\"\n",
    "<html><head><meta charset='utf-8'><title>Forecast Report</title></head><body>\n",
    "<h2>Forecast evaluation (prefix {T_pref} → total {T})</h2>\n",
    "<p>Generated: {datetime.utcnow().isoformat()}Z</p>\n",
    "<table border='1' cellspacing='0' cellpadding='6'>\n",
    "<tr><th>Model</th><th>RMSE</th><th>sMAPE</th><th>N eval wells</th></tr>\n",
    "<tr><td>KNN</td><td>{metrics_knn['rmse']:.4f}</td><td>{metrics_knn['smape']:.4f}</td><td>{metrics_knn['n_eval']}</td></tr>\n",
    "<tr><td>ElasticNet</td><td>{metrics_lr['rmse']:.4f}</td><td>{metrics_lr['smape']:.4f}</td><td>{metrics_lr['n_eval']}</td></tr>\n",
    "</table>\n",
    "<h3>Files</h3>\n",
    "<ul>\n",
    "  <li>{metrics_path.name}</li>\n",
    "  <li>{suffix_paths['knn'].name}</li>\n",
    "  <li>{suffix_paths['elasticnet'].name}</li>\n",
    "  <li>{full_paths['knn'].name}</li>\n",
    "  <li>{full_paths['elasticnet'].name}</li>\n",
    "  <li>{long_path.name}</li>\n",
    "</ul>\n",
    "<h3>Full-series examples</h3>\n",
    "{''.join(f\"<img src='{img.name}' style='max-width:640px;display:block;margin-bottom:10px;'/>\" for img in example_imgs)}\n",
    "</body></html>\n",
    "\"\"\"\n",
    "    report_path = output_dir / \"forecast_report.html\"\n",
    "    report_path.write_text(html, encoding=\"utf-8\")\n",
    "\n",
    "    print(\"KNN   → RMSE={rmse:.4f}, sMAPE={smape:.4f}, N={n_eval}\".format(**metrics_knn))\n",
    "    print(\"ENet  → RMSE={rmse:.4f}, sMAPE={smape:.4f}, N={n_eval}\".format(**metrics_lr))\n",
    "    print(\"Артефакты сохранены в:\", output_dir)\n",
    "\n",
    "    return {\n",
    "        \"metrics\": metrics_df,\n",
    "        \"metrics_path\": metrics_path,\n",
    "        \"suffix_paths\": suffix_paths,\n",
    "        \"full_paths\": full_paths,\n",
    "        \"long_path\": long_path,\n",
    "        \"example_imgs\": example_imgs,\n",
    "        \"report_path\": report_path,\n",
    "        \"info\": {\"knn\": knn_info, \"enet\": lr_info},\n",
    "    }\n",
    "\n",
    "forecast_artifacts = None\n",
    "if RUN_FORECAST and preproc_out is not None:\n",
    "    forecast_artifacts = run_forecast_pipeline(\n",
    "        preproc_out,\n",
    "        T_pref=FORECAST_PREFIX,\n",
    "        output_dir=FORECAST_EXPORT_DIR,\n",
    "        rng_seed=RANDOM_SEED,\n",
    "    )\n",
    "pipeline_state[\"forecast\"] = forecast_artifacts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Сводка состояния пайплайна\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def summarize_state(state: Dict[str, Any]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for key, value in state.items():\n",
    "        rows.append({\n",
    "            \"step\": key,\n",
    "            \"available\": value is not None,\n",
    "            \"type\": type(value).__name__,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "if pipeline_state:\n",
    "    display(summarize_state(pipeline_state))\n",
    "else:\n",
    "    print(\"pipeline_state пуст — проверьте выполнение предыдущих ячеек.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}