{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Behavior Manifold & Forecast\n",
    "\n",
    "Обновлённый ноутбук объединяет основные шаги пайплайна PBM в единое рабочее место:\n",
    "1. Загрузка и очистка сырых ежемесячных отчётов.\n",
    "2. Предобработка профилей (Step 1) и сохранение результатов.\n",
    "3. Построение дескрипторов, manifold-embedding и кластеризация.\n",
    "4. Генерация отчётов по сегментации.\n",
    "5. Построение прогнозов по префиксу (20 → 100).\n",
    "\n",
    "Ниже каждая секция изолирована функциями и настройками, что упрощает повторный запуск и адаптацию под другие проекты.\n"
   ],
   "id": "a7ded500dc7ef217"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Импорт, конфигурация и утилиты\n"
   ],
   "id": "6f7b91338b73191d"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T08:20:53.025121Z",
     "start_time": "2025-09-18T08:20:52.157581Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"SCIPY_ARRAY_API\"] = \"1\"\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.width = 120\n"
   ],
   "id": "bb22864aed3e8493",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T08:20:54.364194Z",
     "start_time": "2025-09-18T08:20:54.362124Z"
    }
   },
   "source": [
    "# Базовые пути и параметры выполнения\n",
    "DATA_DIR = Path(\"data/wells\")\n",
    "FORECAST_EXPORT_DIR = Path(\"reports/forecast_exports\")\n",
    "PBM_REPORT_DIR = Path(\"reports/pbm_report_exports\")\n",
    "\n",
    "# Ограничители для быстрых прогонов (None -> использовать всё)\n",
    "MAX_CSV_FILES: Optional[int] = 500   # например, 5 чтобы взять только первые файлы\n",
    "SAMPLE_WELLS: Optional[int] = 500    # например, 500 чтобы ограничить число скважин\n",
    "\n",
    "# Параметры шагов\n",
    "RUN_MANIFOLD = True\n",
    "RUN_CLUSTERING = True\n",
    "RUN_PBM_REPORT = True  # отчёт требует matplotlib/hdbscan и может быть тяжёлым\n",
    "RUN_FORECAST = True\n",
    "\n",
    "FORECAST_PREFIX = 20\n",
    "RANDOM_SEED = 59\n",
    "\n",
    "pipeline_state: Dict[str, Any] = {}\n"
   ],
   "id": "5adf0b189a0085da",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T08:20:56.320441Z",
     "start_time": "2025-09-18T08:20:56.314379Z"
    }
   },
   "source": [
    "def discover_csv_files(data_dir: Path, limit: Optional[int] = None) -> list[Path]:\n",
    "    \"\"\"Поиск CSV с исходными данными.\"\"\"\n",
    "    if not data_dir.exists():\n",
    "        raise FileNotFoundError(f\"Каталог {data_dir} не найден.\")\n",
    "    csv_files = sorted(data_dir.glob(\"*.csv\"))\n",
    "    if limit is not None:\n",
    "        csv_files = csv_files[:limit]\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"В каталоге {data_dir} нет csv-файлов.\")\n",
    "    return csv_files\n",
    "\n",
    "RENAME_MAP = {\n",
    "    \"BBLS_OIL_COND\": \"oil\",\n",
    "    \"MCF_GAS\": \"gas\",\n",
    "    \"BBLS_WTR\": \"water\",\n",
    "    \"API_WellNo\": \"well_name\",\n",
    "    \"RptDate\": \"date\",\n",
    "    \"DAYS_PROD\": \"days_prod\",\n",
    "}\n",
    "\n",
    "def load_raw_well_data(data_dir: Path, limit_files: Optional[int] = None) -> pd.DataFrame:\n",
    "    csv_files = discover_csv_files(data_dir, limit_files)\n",
    "    frames = [pd.read_csv(path) for path in csv_files]\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    print(f\"Считано файлов: {len(csv_files)} — строк: {len(df):,}\")\n",
    "    return df\n",
    "\n",
    "def clean_well_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.rename(columns=RENAME_MAP)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    drop_cols = [c for c in (\"Lease_Unit\", \"Formation\") if c in df.columns]\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols)\n",
    "    df = df[(df[\"oil\"] >= 0) & (df[\"gas\"] >= 0) & (df[\"water\"] >= 0)].copy()\n",
    "    df = df.sort_values(by=[\"well_name\", \"date\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def subset_wells(df: pd.DataFrame, n_wells: Optional[int] = None) -> pd.DataFrame:\n",
    "    if n_wells is None:\n",
    "        return df\n",
    "    wells = (\n",
    "        df[\"well_name\"]\n",
    "        .dropna()\n",
    "        .drop_duplicates()\n",
    "        .sort_values()\n",
    "        .head(n_wells)\n",
    "    )\n",
    "    return df[df[\"well_name\"].isin(wells)].copy()\n",
    "\n",
    "def prepare_dataset(data_dir: Path, limit_files: Optional[int] = None, sample_wells: Optional[int] = None) -> pd.DataFrame:\n",
    "    raw = load_raw_well_data(data_dir, limit_files)\n",
    "    cleaned = clean_well_data(raw)\n",
    "    subset = subset_wells(cleaned, sample_wells)\n",
    "    print(f\"Итоговый датафрейм: {subset.shape[0]:,} строк, {subset['well_name'].nunique()} скважин\")\n",
    "    return subset\n"
   ],
   "id": "6cdedb6391168469",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T08:20:57.751790Z",
     "start_time": "2025-09-18T08:20:57.330198Z"
    }
   },
   "source": [
    "try:\n",
    "    df = prepare_dataset(DATA_DIR, MAX_CSV_FILES, SAMPLE_WELLS)\n",
    "except FileNotFoundError as exc:\n",
    "    print(f\"⚠️ {exc}\")\n",
    "    df = None\n",
    "else:\n",
    "    pipeline_state[\"dataframe\"] = df\n",
    "    display(df.head())\n"
   ],
   "id": "e74cb6ee288346f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Считано файлов: 500 — строк: 132,457\n",
      "Итоговый датафрейм: 132,441 строк, 475 скважин\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "            well_name       date  oil  gas  water  days_prod\n",
       "0  25-003-05057-00-00 1986-01-31  149    0   1788       31.0\n",
       "1  25-003-05057-00-00 1986-03-31  210    0   2520       31.0\n",
       "2  25-003-05057-00-00 1986-04-30  252    0   3024       31.0\n",
       "3  25-003-05057-00-00 1986-05-31  264    0   3172       31.0\n",
       "4  25-003-05057-00-00 1986-06-30  192    0   2296       31.0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>well_name</th>\n",
       "      <th>date</th>\n",
       "      <th>oil</th>\n",
       "      <th>gas</th>\n",
       "      <th>water</th>\n",
       "      <th>days_prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25-003-05057-00-00</td>\n",
       "      <td>1986-01-31</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>1788</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25-003-05057-00-00</td>\n",
       "      <td>1986-03-31</td>\n",
       "      <td>210</td>\n",
       "      <td>0</td>\n",
       "      <td>2520</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25-003-05057-00-00</td>\n",
       "      <td>1986-04-30</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>3024</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25-003-05057-00-00</td>\n",
       "      <td>1986-05-31</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>3172</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25-003-05057-00-00</td>\n",
       "      <td>1986-06-30</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>2296</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Предобработка профилей (PBM Step 1)\n"
   ],
   "id": "71d57e7c5eaebde"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T08:22:17.340605Z",
     "start_time": "2025-09-18T08:22:16.709263Z"
    }
   },
   "source": [
    "from tools.preprocessing import PreprocConfig, preprocess_profiles\n",
    "\n",
    "def run_preprocessing(df: pd.DataFrame, cfg: Optional[PreprocConfig] = None) -> Dict[str, Any]:\n",
    "    if df is None:\n",
    "        raise ValueError(\"Нет исходных данных — убедитесь, что предыдущая ячейка выполнена успешно.\")\n",
    "    cfg = cfg or PreprocConfig()\n",
    "    print(f\"Запуск preprocess_profiles для {df['well_name'].nunique()} скважин (T={cfg.T})\")\n",
    "    out = preprocess_profiles(df, cfg)\n",
    "    print(\"panel_long shape:\", out[\"panel_long\"].shape)\n",
    "    print(\"tensor shape:\", out[\"X\"].shape)\n",
    "    display(out[\"panel_long\"].head(12))\n",
    "    out[\"config\"] = dict(out.get(\"config\", {}))\n",
    "    return out\n",
    "\n",
    "preproc_cfg = PreprocConfig()\n",
    "preproc_out = run_preprocessing(df, preproc_cfg) if df is not None else None\n",
    "pipeline_state[\"preprocessing\"] = preproc_out\n",
    "out = preproc_out  # совместимость со старыми скриптами\n"
   ],
   "id": "973d85a466aef37f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск preprocess_profiles для 475 скважин (T=70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Сформировано: N=294, T=70, C=4. Каналы: ['r_oil_norm', 'wc', 'gor', 'dr_oil_norm']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m     13\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[32m     15\u001B[39m preproc_cfg = PreprocConfig()\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m preproc_out = \u001B[43mrun_preprocessing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreproc_cfg\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m df \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     17\u001B[39m pipeline_state[\u001B[33m\"\u001B[39m\u001B[33mpreprocessing\u001B[39m\u001B[33m\"\u001B[39m] = preproc_out\n\u001B[32m     18\u001B[39m out = preproc_out  \u001B[38;5;66;03m# совместимость со старыми скриптами\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 9\u001B[39m, in \u001B[36mrun_preprocessing\u001B[39m\u001B[34m(df, cfg)\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mЗапуск preprocess_profiles для \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdf[\u001B[33m'\u001B[39m\u001B[33mwell_name\u001B[39m\u001B[33m'\u001B[39m].nunique()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m скважин (T=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcfg.T\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m)\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      8\u001B[39m out = preprocess_profiles(df, cfg)\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mpanel_long shape:\u001B[39m\u001B[33m\"\u001B[39m, \u001B[43mout\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpanel_long\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m.shape)\n\u001B[32m     10\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mtensor shape:\u001B[39m\u001B[33m\"\u001B[39m, out[\u001B[33m\"\u001B[39m\u001B[33mX\u001B[39m\u001B[33m\"\u001B[39m].shape)\n\u001B[32m     11\u001B[39m display(out[\u001B[33m\"\u001B[39m\u001B[33mpanel_long\u001B[39m\u001B[33m\"\u001B[39m].head(\u001B[32m12\u001B[39m))\n",
      "\u001B[31mTypeError\u001B[39m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2–3. Компактные признаки и manifold\n"
   ],
   "id": "48151f649c7c9db"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T19:01:51.172231Z",
     "start_time": "2025-09-17T19:00:22.996510Z"
    }
   },
   "source": [
    "try:\n",
    "    from tools.features import compute_side_features, scale_features\n",
    "    from tools.manifold import ManifoldConfig, embed_umap_euclid, embed_umap_fastdtw\n",
    "    MANIFOLD_IMPORT_ERROR = None\n",
    "except Exception as exc:\n",
    "    MANIFOLD_IMPORT_ERROR = exc\n",
    "\n",
    "def run_manifold(out_dict: Dict[str, Any], cfg: Optional[ManifoldConfig] = None, sample_size: Optional[int] = None) -> Dict[str, Any]:\n",
    "    if out_dict is None:\n",
    "        raise ValueError(\"Нет результатов предобработки.\")\n",
    "    if MANIFOLD_IMPORT_ERROR is not None:\n",
    "        raise RuntimeError(f\"Не удалось импортировать инструменты manifold: {MANIFOLD_IMPORT_ERROR}\")\n",
    "    cfg = cfg or ManifoldConfig()\n",
    "    panel_long = out_dict[\"panel_long\"]\n",
    "    X = out_dict[\"X\"]\n",
    "    wells = out_dict[\"wells_used\"]\n",
    "    tensor_channels = out_dict[\"tensor_channels\"]\n",
    "    T = int(out_dict.get(\"config\", {}).get(\"T\", X.shape[1]))\n",
    "\n",
    "    feats = compute_side_features(panel_long, T=T)\n",
    "    feats_scaled, scaler = scale_features(feats)\n",
    "    print(\"Side features shape:\", feats_scaled.shape)\n",
    "\n",
    "    Z_euclid, umap_e = embed_umap_euclid(\n",
    "        X,\n",
    "        tensor_channels=tensor_channels,\n",
    "        channels=cfg.channels,\n",
    "        n_neighbors=cfg.n_neighbors,\n",
    "        min_dist=cfg.min_dist,\n",
    "        n_components=cfg.n_components,\n",
    "        random_state=cfg.random_state,\n",
    "    )\n",
    "    print(\"UMAP (euclid) shape:\", Z_euclid.shape)\n",
    "\n",
    "    Z_dtw, sub_idx, dist_matrix, info = embed_umap_fastdtw(\n",
    "        X,\n",
    "        tensor_channels=tensor_channels,\n",
    "        channels=cfg.channels,\n",
    "        cfg=cfg,\n",
    "        sample_size=sample_size,\n",
    "        # candidate_knn можно не передавать: возьмется max(k_refine, n_neighbors)\n",
    "    )\n",
    "\n",
    "    wells_sub = np.array(wells)[sub_idx]\n",
    "    embedding_df = pd.DataFrame({\n",
    "        \"well_name\": wells_sub,\n",
    "        \"x\": Z_dtw[:, 0],\n",
    "        \"y\": Z_dtw[:, 1],\n",
    "    })\n",
    "    display(embedding_df.head())\n",
    "    return {\n",
    "        \"cfg\": cfg,\n",
    "        \"features\": feats,\n",
    "        \"features_scaled\": feats_scaled,\n",
    "        \"scaler\": scaler,\n",
    "        \"Z_euclid\": Z_euclid,\n",
    "        \"Z_dtw\": Z_dtw,\n",
    "        \"sub_idx\": sub_idx,\n",
    "        \"dist_matrix\": dist_matrix,\n",
    "        \"info\": info,\n",
    "        \"embedding_df\": embedding_df,\n",
    "    }\n",
    "\n",
    "manifold_cfg = None\n",
    "manifold_out = None\n",
    "if RUN_MANIFOLD and preproc_out is not None:\n",
    "    manifold_cfg = ManifoldConfig(channels=(\"r_oil_norm\", \"wc\"), random_state=RANDOM_SEED)\n",
    "    manifold_out = run_manifold(preproc_out, manifold_cfg)\n",
    "pipeline_state[\"manifold\"] = manifold_out\n"
   ],
   "id": "19f10835f1e16b18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Side features shape: (290, 16)\n",
      "UMAP (euclid) shape: (290, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recompute DTW for 7918 pairs (radius=6): 100%|██████████| 7918/7918 [01:26<00:00, 91.32it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "            well_name         x          y\n",
       "0  25-003-05057-00-00  5.201151  12.319011\n",
       "1  25-003-05068-00-00  5.160582  12.275242\n",
       "2  25-003-21142-00-00  9.550284  10.671461\n",
       "3  25-003-21143-00-00  8.711246  11.115558\n",
       "4  25-005-08003-00-00  5.027185  11.693102"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>well_name</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25-003-05057-00-00</td>\n",
       "      <td>5.201151</td>\n",
       "      <td>12.319011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25-003-05068-00-00</td>\n",
       "      <td>5.160582</td>\n",
       "      <td>12.275242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25-003-21142-00-00</td>\n",
       "      <td>9.550284</td>\n",
       "      <td>10.671461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25-003-21143-00-00</td>\n",
       "      <td>8.711246</td>\n",
       "      <td>11.115558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25-005-08003-00-00</td>\n",
       "      <td>5.027185</td>\n",
       "      <td>11.693102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Кластеризация и поиск аномалий\n"
   ],
   "id": "99252f9e6c0dae14"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T06:58:36.441318Z",
     "start_time": "2025-09-16T06:58:18.696738Z"
    }
   },
   "source": [
    "try:\n",
    "    from tools.clustering import (\n",
    "        ClusterConfig,\n",
    "        assign_anomaly_scores,\n",
    "        build_cluster_prototypes,\n",
    "        cluster_hdbscan,\n",
    "        summarize_clusters,\n",
    "    )\n",
    "    CLUSTER_IMPORT_ERROR = None\n",
    "except Exception as exc:\n",
    "    CLUSTER_IMPORT_ERROR = exc\n",
    "\n",
    "def run_clustering(out_dict: Dict[str, Any], manifold_dict: Dict[str, Any], cfg: Optional[ClusterConfig] = None) -> Optional[Dict[str, Any]]:\n",
    "    if out_dict is None or manifold_dict is None:\n",
    "        print(\"Кластеризация пропущена — нет данных manifold.\")\n",
    "        return None\n",
    "    if CLUSTER_IMPORT_ERROR is not None:\n",
    "        raise RuntimeError(f\"Не удалось импортировать инструменты кластеризации: {CLUSTER_IMPORT_ERROR}\")\n",
    "    cfg = cfg or ClusterConfig(min_cluster_size=45, min_samples=12)\n",
    "    embedding = manifold_dict.get(\"Z_dtw\")\n",
    "    sub_idx = manifold_dict.get(\"sub_idx\")\n",
    "    if embedding is None or sub_idx is None:\n",
    "        embedding = manifold_dict[\"Z_euclid\"]\n",
    "        sub_idx = np.arange(embedding.shape[0])\n",
    "    wells_sub = np.array(out_dict[\"wells_used\"])[sub_idx]\n",
    "    res = cluster_hdbscan(embedding, wells_sub.tolist(), cfg)\n",
    "    df_map = res[\"df_map\"]\n",
    "    df_map = assign_anomaly_scores(df_map, embedding, res[\"labels\"], lof_k=35)\n",
    "    summary = summarize_clusters(df_map)\n",
    "    display(summary)\n",
    "    prototypes = build_cluster_prototypes(\n",
    "        out_dict[\"panel_long\"],\n",
    "        df_map,\n",
    "        channels=(\"r_oil_s\", \"wc\", \"gor\", \"r_oil_norm\"),\n",
    "        T=int(out_dict[\"config\"][\"T\"]),\n",
    "        method=\"auto\",\n",
    "    )\n",
    "    print(f\"Silhouette={res['silhouette']:.3f}, DBCV={res['dbcv']:.3f}\")\n",
    "    return {\n",
    "        \"cfg\": cfg,\n",
    "        \"result\": res,\n",
    "        \"df_map\": df_map,\n",
    "        \"summary\": summary,\n",
    "        \"prototypes\": prototypes,\n",
    "    }\n",
    "\n",
    "cluster_cfg = None\n",
    "cluster_out = None\n",
    "if RUN_CLUSTERING and manifold_out is not None:\n",
    "    cluster_cfg = ClusterConfig(min_cluster_size=45, min_samples=12)\n",
    "    cluster_out = run_clustering(preproc_out, manifold_out, cluster_cfg)\n",
    "pipeline_state[\"clustering\"] = cluster_out\n"
   ],
   "id": "788058ea6950fccf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   cluster  size     share  prob_median\n",
       "0        0   204  0.384181     0.789915\n",
       "1        1   327  0.615819     1.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>size</th>\n",
       "      <th>share</th>\n",
       "      <th>prob_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>204</td>\n",
       "      <td>0.384181</td>\n",
       "      <td>0.789915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>327</td>\n",
       "      <td>0.615819</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette=0.717, DBCV=nan\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Экспорт отчёта PBM\n"
   ],
   "id": "c60090bdc189aa89"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T06:58:37.234738Z",
     "start_time": "2025-09-16T06:58:36.449983Z"
    }
   },
   "source": [
    "try:\n",
    "    from tools.make_reports import (\n",
    "        build_html_report,\n",
    "        export_csv_summaries,\n",
    "        save_cluster_distribution_plot,\n",
    "        save_cluster_prototype_plots,\n",
    "        save_pbm_map,\n",
    "    )\n",
    "    REPORT_IMPORT_ERROR = None\n",
    "except Exception as exc:\n",
    "    REPORT_IMPORT_ERROR = exc\n",
    "\n",
    "def build_pbm_report(preproc_dict: Dict[str, Any], manifold_dict: Dict[str, Any], cluster_dict: Dict[str, Any], out_dir: Path) -> Dict[str, Any]:\n",
    "    if cluster_dict is None:\n",
    "        print(\"Отчёт пропущен — кластеризация не выполнена.\")\n",
    "        return {}\n",
    "    if REPORT_IMPORT_ERROR is not None:\n",
    "        raise RuntimeError(f\"Не удалось импортировать генераторы отчётов: {REPORT_IMPORT_ERROR}\")\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    embedding = manifold_dict.get(\"Z_dtw\")\n",
    "    if embedding is None:\n",
    "        embedding = manifold_dict[\"Z_euclid\"]\n",
    "    df_map = cluster_dict[\"df_map\"]\n",
    "    map_png = save_pbm_map(embedding, df_map, str(out_dir))\n",
    "    sizes_png = save_cluster_distribution_plot(df_map, str(out_dir))\n",
    "    T = int(preproc_dict[\"config\"][\"T\"])\n",
    "    proto_pngs = save_cluster_prototype_plots(\n",
    "        preproc_dict[\"panel_long\"],\n",
    "        df_map,\n",
    "        cluster_dict[\"prototypes\"],\n",
    "        channels=(\"r_oil_s\", \"wc\", \"gor\", \"r_oil_norm\"),\n",
    "        T=T,\n",
    "        out_dir=str(out_dir),\n",
    "    )\n",
    "    summary = cluster_dict[\"summary\"]\n",
    "    csv_paths = export_csv_summaries(df_map, summary, str(out_dir), top_anoms=50)\n",
    "    report_path = build_html_report(\n",
    "        str(out_dir),\n",
    "        map_png,\n",
    "        sizes_png,\n",
    "        proto_pngs,\n",
    "        df_map,\n",
    "        summary,\n",
    "        title=\"PBM Report\",\n",
    "    )\n",
    "    print(\"Отчёт сохранён:\", report_path)\n",
    "    return {\n",
    "        \"out_dir\": out_dir,\n",
    "        \"map_png\": map_png,\n",
    "        \"sizes_png\": sizes_png,\n",
    "        \"proto_pngs\": proto_pngs,\n",
    "        \"csv_paths\": csv_paths,\n",
    "        \"report_path\": report_path,\n",
    "    }\n",
    "\n",
    "pbm_report = None\n",
    "if RUN_PBM_REPORT and cluster_out is not None:\n",
    "    pbm_report = build_pbm_report(preproc_out, manifold_out, cluster_out, PBM_REPORT_DIR)\n",
    "pipeline_state[\"pbm_report\"] = pbm_report\n"
   ],
   "id": "f89b3556c4ea1270",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отчёт сохранён: reports/pbm_report_exports/PBM_report.html\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Прогноз профиля (20 → 100)",
   "id": "69953d34698259ef"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T06:58:39.638304Z",
     "start_time": "2025-09-16T06:58:37.243417Z"
    }
   },
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from tools.forecast import (\n",
    "    build_prefix_scaled_channel,\n",
    "    evaluate_forecasts,\n",
    "    knn_forecast,\n",
    "    make_matrices,\n",
    "    multioutput_forecast,\n",
    ")\n",
    "\n",
    "def run_forecast_pipeline(out_dict: Dict[str, Any], T_pref: int, output_dir: Path, rng_seed: int = 59) -> Dict[str, Any]:\n",
    "    if out_dict is None:\n",
    "        raise ValueError(\"Нет результатов предобработки.\")\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    panel_long = out_dict[\"panel_long\"].copy()\n",
    "    wells_used = out_dict[\"wells_used\"]\n",
    "    T = int(out_dict[\"config\"][\"T\"])\n",
    "\n",
    "    panel_long = build_prefix_scaled_channel(\n",
    "        panel_long,\n",
    "        wells_used,\n",
    "        T=T,\n",
    "        T_pref=T_pref,\n",
    "        q=0.90,\n",
    "        rate_col=\"r_oil_s\",\n",
    "        out_col=\"r_oil_pref_norm\",\n",
    "    )\n",
    "\n",
    "    X_pref, Y_suffix_true, Y_full = make_matrices(\n",
    "        panel_long,\n",
    "        wells_used,\n",
    "        T=T,\n",
    "        T_pref=T_pref,\n",
    "        channel=\"r_oil_pref_norm\",\n",
    "        target_col=\"r_oil_s\",\n",
    "    )\n",
    "\n",
    "    Y_pred_knn, knn_info = knn_forecast(X_pref, Y_full, T_pref=T_pref, K=15)\n",
    "    Y_pred_lr, lr_info = multioutput_forecast(panel_long, wells_used, T=T, T_pref=T_pref, Y_full=Y_full, random_state=43)\n",
    "\n",
    "    metrics_knn = evaluate_forecasts(Y_suffix_true, Y_pred_knn)\n",
    "    metrics_lr = evaluate_forecasts(Y_suffix_true, Y_pred_lr)\n",
    "\n",
    "    metrics_df = pd.DataFrame([\n",
    "        {\"model\": \"knn\", **metrics_knn},\n",
    "        {\"model\": \"elasticnet\", **metrics_lr},\n",
    "    ])\n",
    "    metrics_path = output_dir / \"metrics.csv\"\n",
    "    metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "    np.save(output_dir / \"Y_suffix_true.npy\", Y_suffix_true)\n",
    "    np.save(output_dir / \"Y_pred_knn.npy\", Y_pred_knn)\n",
    "    np.save(output_dir / \"Y_pred_enet.npy\", Y_pred_lr)\n",
    "\n",
    "    suffix_cols = [f\"m{t}\" for t in range(T_pref + 1, T + 1)]\n",
    "\n",
    "    def save_suffix_csv(arr: np.ndarray, name: str) -> Path:\n",
    "        df_pred = pd.DataFrame(arr, columns=suffix_cols)\n",
    "        df_pred.insert(0, \"well_name\", wells_used)\n",
    "        path = output_dir / f\"pred_{name}.csv\"\n",
    "        df_pred.to_csv(path, index=False)\n",
    "        return path\n",
    "\n",
    "    suffix_paths = {\n",
    "        \"knn\": save_suffix_csv(Y_pred_knn, \"knn\"),\n",
    "        \"elasticnet\": save_suffix_csv(Y_pred_lr, \"elasticnet\"),\n",
    "    }\n",
    "\n",
    "    full_cols = [f\"m{t+1}\" for t in range(T)]\n",
    "\n",
    "    def save_full_csv(arr: np.ndarray, name: str) -> Path:\n",
    "        df_pred = pd.DataFrame(arr, columns=full_cols)\n",
    "        df_pred.insert(0, \"well_name\", wells_used)\n",
    "        path = output_dir / f\"pred_full_{name}.csv\"\n",
    "        df_pred.to_csv(path, index=False)\n",
    "        return path\n",
    "\n",
    "    Y_hat_knn_full = Y_full.copy()\n",
    "    Y_hat_enet_full = Y_full.copy()\n",
    "    Y_hat_knn_full[:, T_pref:T] = Y_pred_knn\n",
    "    Y_hat_enet_full[:, T_pref:T] = Y_pred_lr\n",
    "\n",
    "    full_paths = {\n",
    "        \"knn\": save_full_csv(Y_hat_knn_full, \"knn\"),\n",
    "        \"elasticnet\": save_full_csv(Y_hat_enet_full, \"elasticnet\"),\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    for i, well in enumerate(wells_used):\n",
    "        for t in range(T):\n",
    "            segment = \"observed\" if t < T_pref else \"forecast\"\n",
    "            rows.append({\n",
    "                \"well_name\": well,\n",
    "                \"t\": t,\n",
    "                \"y_true\": float(Y_full[i, t]) if np.isfinite(Y_full[i, t]) else np.nan,\n",
    "                \"y_pred_knn\": float(Y_hat_knn_full[i, t]) if np.isfinite(Y_hat_knn_full[i, t]) else np.nan,\n",
    "                \"y_pred_elasticnet\": float(Y_hat_enet_full[i, t]) if np.isfinite(Y_hat_enet_full[i, t]) else np.nan,\n",
    "                \"segment\": segment,\n",
    "            })\n",
    "    long_df = pd.DataFrame(rows)\n",
    "    long_path = output_dir / \"pred_long.csv\"\n",
    "    long_df.to_csv(long_path, index=False)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def plot_full_example(idx: int, title_prefix: str, pred_full: np.ndarray) -> Path:\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        ax.plot(range(T), Y_full[idx], label=\"true\")\n",
    "        ax.plot(range(T), pred_full[idx], label=\"pred\")\n",
    "        ax.axvline(T_pref - 1, linestyle=\"--\")\n",
    "        ax.set_title(f\"{title_prefix} — {wells_used[idx]}\")\n",
    "        ax.set_xlabel(\"month index\")\n",
    "        ax.set_ylabel(\"oil rate (r_oil_s)\")\n",
    "        ax.legend()\n",
    "        path = output_dir / f\"{title_prefix.replace(' ', '_').lower()}_{idx}_full.png\"\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(path, dpi=140)\n",
    "        plt.close(fig)\n",
    "        return path\n",
    "\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    valid_idx = np.where(np.isfinite(Y_pred_knn).all(axis=1))[0]\n",
    "    example_imgs: list[Path] = []\n",
    "    if valid_idx.size:\n",
    "        chosen = rng.choice(valid_idx, size=min(6, valid_idx.size), replace=False)\n",
    "        for idx in chosen:\n",
    "            example_imgs.append(plot_full_example(idx, \"knn_example\", Y_hat_knn_full))\n",
    "            example_imgs.append(plot_full_example(idx, \"enet_example\", Y_hat_enet_full))\n",
    "\n",
    "    html = f\"\"\"\n",
    "<html><head><meta charset='utf-8'><title>Forecast Report</title></head><body>\n",
    "<h2>Forecast evaluation (prefix {T_pref} → total {T})</h2>\n",
    "<p>Generated: {datetime.utcnow().isoformat()}Z</p>\n",
    "<table border='1' cellspacing='0' cellpadding='6'>\n",
    "<tr><th>Model</th><th>RMSE</th><th>sMAPE</th><th>N eval wells</th></tr>\n",
    "<tr><td>KNN</td><td>{metrics_knn['rmse']:.4f}</td><td>{metrics_knn['smape']:.4f}</td><td>{metrics_knn['n_eval']}</td></tr>\n",
    "<tr><td>ElasticNet</td><td>{metrics_lr['rmse']:.4f}</td><td>{metrics_lr['smape']:.4f}</td><td>{metrics_lr['n_eval']}</td></tr>\n",
    "</table>\n",
    "<h3>Files</h3>\n",
    "<ul>\n",
    "  <li>{metrics_path.name}</li>\n",
    "  <li>{suffix_paths['knn'].name}</li>\n",
    "  <li>{suffix_paths['elasticnet'].name}</li>\n",
    "  <li>{full_paths['knn'].name}</li>\n",
    "  <li>{full_paths['elasticnet'].name}</li>\n",
    "  <li>{long_path.name}</li>\n",
    "</ul>\n",
    "<h3>Full-series examples</h3>\n",
    "{''.join(f\"<img src='{img.name}' style='max-width:640px;display:block;margin-bottom:10px;'/>\" for img in example_imgs)}\n",
    "</body></html>\n",
    "\"\"\"\n",
    "    report_path = output_dir / \"forecast_report.html\"\n",
    "    report_path.write_text(html, encoding=\"utf-8\")\n",
    "\n",
    "    print(\"KNN   → RMSE={rmse:.4f}, sMAPE={smape:.4f}, N={n_eval}\".format(**metrics_knn))\n",
    "    print(\"ENet  → RMSE={rmse:.4f}, sMAPE={smape:.4f}, N={n_eval}\".format(**metrics_lr))\n",
    "    print(\"Артефакты сохранены в:\", output_dir)\n",
    "\n",
    "    return {\n",
    "        \"metrics\": metrics_df,\n",
    "        \"metrics_path\": metrics_path,\n",
    "        \"suffix_paths\": suffix_paths,\n",
    "        \"full_paths\": full_paths,\n",
    "        \"long_path\": long_path,\n",
    "        \"example_imgs\": example_imgs,\n",
    "        \"report_path\": report_path,\n",
    "        \"info\": {\"knn\": knn_info, \"enet\": lr_info},\n",
    "    }\n",
    "\n",
    "forecast_artifacts = None\n",
    "if RUN_FORECAST and preproc_out is not None:\n",
    "    forecast_artifacts = run_forecast_pipeline(\n",
    "        preproc_out,\n",
    "        T_pref=FORECAST_PREFIX,\n",
    "        output_dir=FORECAST_EXPORT_DIR,\n",
    "        rng_seed=RANDOM_SEED,\n",
    "    )\n",
    "pipeline_state[\"forecast\"] = forecast_artifacts\n"
   ],
   "id": "ad6780a26ce54537",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN   → RMSE=14.7542, sMAPE=0.5209, N=469\n",
      "ENet  → RMSE=29.1081, sMAPE=1.1633, N=469\n",
      "Артефакты сохранены в: reports/forecast_exports\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Сводка состояния пайплайна\n"
   ],
   "id": "de72e1864f823f2f"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T06:58:39.649829Z",
     "start_time": "2025-09-16T06:58:39.646945Z"
    }
   },
   "source": [
    "def summarize_state(state: Dict[str, Any]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for key, value in state.items():\n",
    "        rows.append({\n",
    "            \"step\": key,\n",
    "            \"available\": value is not None,\n",
    "            \"type\": type(value).__name__,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "if pipeline_state:\n",
    "    display(summarize_state(pipeline_state))\n",
    "else:\n",
    "    print(\"pipeline_state пуст — проверьте выполнение предыдущих ячеек.\")\n"
   ],
   "id": "89686a88539e8c8e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            step  available       type\n",
       "0      dataframe       True  DataFrame\n",
       "1  preprocessing       True       dict\n",
       "2       manifold       True       dict\n",
       "3     clustering       True       dict\n",
       "4     pbm_report       True       dict\n",
       "5       forecast       True       dict"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>available</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataframe</td>\n",
       "      <td>True</td>\n",
       "      <td>DataFrame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>preprocessing</td>\n",
       "      <td>True</td>\n",
       "      <td>dict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>manifold</td>\n",
       "      <td>True</td>\n",
       "      <td>dict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clustering</td>\n",
       "      <td>True</td>\n",
       "      <td>dict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pbm_report</td>\n",
       "      <td>True</td>\n",
       "      <td>dict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>forecast</td>\n",
       "      <td>True</td>\n",
       "      <td>dict</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
