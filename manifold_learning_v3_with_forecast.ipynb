{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-14T11:32:21.140358Z",
     "start_time": "2025-09-14T11:32:20.995689Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"SCIPY_ARRAY_API\"] = \"1\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "9f07aae0df2425f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:32:46.255348Z",
     "start_time": "2025-09-14T11:32:21.145540Z"
    }
   },
   "source": [
    "# путь к папке\n",
    "folder = \"data\"\n",
    "\n",
    "# ищем все csv-файлы\n",
    "all_csv = glob.glob(os.path.join(folder, \"*.csv\"))\n",
    "\n",
    "# читаем и объединяем\n",
    "df_0 = pd.concat((pd.read_csv(f) for f in all_csv), ignore_index=True)\n",
    "print(f\"Считано файлов: {len(all_csv)}\")\n",
    "# переименуем для удобства\n",
    "df_0 = df_0.rename(columns={\"BBLS_OIL_COND\":\"oil\", \"MCF_GAS\": 'gas', \"BBLS_WTR\":\"water\", \"API_WellNo\":\"well_name\", \"RptDate\":\"date\", \"DAYS_PROD\":\"days_prod\"})\n",
    "# преобразуем колонку RptDate к datetime\n",
    "df_0[\"date\"] = pd.to_datetime(df_0[\"date\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Считано файлов: 4485\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1ef2cbd8b6c5e63c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:32:46.303110Z",
     "start_time": "2025-09-14T11:32:46.288817Z"
    }
   },
   "source": [
    "df = df_0.drop(columns=[\"Lease_Unit\", \"Formation\"])\n",
    "print(df.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1212779, 6)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "b07c87fa7e4ab996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:32:46.453805Z",
     "start_time": "2025-09-14T11:32:46.373852Z"
    }
   },
   "source": [
    "# важно, чтобы внутри каждой скважины ряды шли строго по времени\n",
    "df = df.sort_values(by=[\"well_name\", \"date\"]).reset_index(drop=True)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "7517e584c5146a5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:32:46.484179Z",
     "start_time": "2025-09-14T11:32:46.459440Z"
    }
   },
   "source": [
    "# Есть записи с отрицательными дебитами. Исключим их, так как работа будет вестись только с добывающими скважинами\n",
    "df = df[(df['oil'] >= 0) & (df['gas'] >= 0) & (df['water'] >= 0)]"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "4a3d244130dae598",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:32:46.494141Z",
     "start_time": "2025-09-14T11:32:46.489503Z"
    }
   },
   "source": [
    "df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  well_name       date  oil  gas  water  days_prod\n",
       "0        25-003-05000-00-00 1991-02-28    0    0      0        0.0\n",
       "1        25-003-05001-00-00 1991-01-31    0    0      0        0.0\n",
       "2        25-003-05001-00-00 1991-03-31    0    0      0        0.0\n",
       "3        25-003-05001-00-00 1991-04-30    0    0      0        0.0\n",
       "4        25-003-05001-00-00 1991-05-31    0    0      0        0.0\n",
       "...                     ...        ...  ...  ...    ...        ...\n",
       "1212774  25-111-21271-00-00 2025-03-31    0    0      0        0.0\n",
       "1212775  25-111-21271-00-00 2025-04-30    0    0      0        0.0\n",
       "1212776  25-111-21271-00-00 2025-05-31   60   16   5505       16.0\n",
       "1212777  25-111-21271-00-00 2025-06-30    0    0      0        0.0\n",
       "1212778  25-111-21271-00-00 2025-07-31    0    0      0        0.0\n",
       "\n",
       "[1212626 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>well_name</th>\n",
       "      <th>date</th>\n",
       "      <th>oil</th>\n",
       "      <th>gas</th>\n",
       "      <th>water</th>\n",
       "      <th>days_prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25-003-05000-00-00</td>\n",
       "      <td>1991-02-28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25-003-05001-00-00</td>\n",
       "      <td>1991-01-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25-003-05001-00-00</td>\n",
       "      <td>1991-03-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25-003-05001-00-00</td>\n",
       "      <td>1991-04-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25-003-05001-00-00</td>\n",
       "      <td>1991-05-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212774</th>\n",
       "      <td>25-111-21271-00-00</td>\n",
       "      <td>2025-03-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212775</th>\n",
       "      <td>25-111-21271-00-00</td>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212776</th>\n",
       "      <td>25-111-21271-00-00</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>60</td>\n",
       "      <td>16</td>\n",
       "      <td>5505</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212777</th>\n",
       "      <td>25-111-21271-00-00</td>\n",
       "      <td>2025-06-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212778</th>\n",
       "      <td>25-111-21271-00-00</td>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1212626 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "87b6d575f3f6a639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:34:08.716666Z",
     "start_time": "2025-09-14T11:32:46.516950Z"
    }
   },
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Production Behavior Manifold (PBM) — Предобработка профилей (Шаг 1)\n",
    "------------------------------------------------------------------\n",
    "Этот блок определяет конфиг и функции предобработки для датафрейма df\n",
    "со столбцами:\n",
    "['well_name', 'date', 'oil', 'water', 'gas', 'days_prod']\n",
    "\n",
    "Основные шаги:\n",
    "- Приведение к среднесуточным дебитам (учёт days_prod, иначе — дни в месяце)\n",
    "- Единая месячная сетка и агрегация\n",
    "- Фильтры качества и робастная обработка выбросов\n",
    "- Сглаживание (Savitzky–Golay)\n",
    "- Производные каналы (wc, gor), нормализация по пику\n",
    "- Выравнивание времени (t=0 — старт производства)\n",
    "- Усечение/паддинг до горизонта T\n",
    "- Подготовка длинного и «tensor» представления\n",
    "\n",
    "Код написан так, чтобы безопасно выполняться даже без df в окружении.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Конфигурация предобработки\n",
    "# ----------------------------\n",
    "\n",
    "@dataclass\n",
    "class PreprocConfig:\n",
    "    # Временная сетка\n",
    "    freq: str = \"MS\"                 # ежемесячно: начало месяца\n",
    "    T: int = 70                      # целевой горизонт в месяцах (паддинг/обрезка)\n",
    "    min_profile_months: int = 12     # минимальная длина валидного профиля (по основному каналу)\n",
    "\n",
    "    # Порог старта производства для выравнивания (t=0)\n",
    "    # Абс. порог для r_oil_s (сглаженный среднесуточный дебит нефти)\n",
    "    prod_threshold_abs: float = 0.01\n",
    "    # Относительный порог от пика r_oil_s (например, 5% от пика)\n",
    "    prod_threshold_rel: float = 0.01\n",
    "\n",
    "    # Сглаживание (Savitzky–Golay)\n",
    "    smooth_window: int = 5           # должно быть нечётным, корректируется автоматически\n",
    "    smooth_poly: int = 2\n",
    "\n",
    "    # Робастная «подрезка» выбросов (winsorize) по КАЖДОЙ скважине\n",
    "    winsorize_low: float = 0.01      # перцентиль снизу\n",
    "    winsorize_high: float = 0.99     # перцентиль сверху\n",
    "\n",
    "    # Нормализация амплитуды (деление на пик r_oil_s)\n",
    "    normalize_by_peak: bool = True\n",
    "    eps: float = 1e-9\n",
    "\n",
    "    # Ограничения и заполнение\n",
    "    clamp_nonnegative: bool = True   # отрицательные объёмы/дебиты → NaN (позже можно 0)\n",
    "    interpolate_gaps: bool = True    # интерполяция пропусков для сглаживания\n",
    "\n",
    "    # Каналы, которые пойдут в итоговый тензор (в таком порядке)\n",
    "    tensor_channels: Tuple[str, ...] = (\n",
    "        \"r_oil_norm\", \"wc\", \"gor\", \"dr_oil_norm\"\n",
    "    )\n",
    "\n",
    "    # Основной канал, по которому считаем длину валидного профиля\n",
    "    primary_channel: str = \"r_oil_s\"\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Утилиты\n",
    "# ----------------------------\n",
    "\n",
    "def _safe_quantiles(s: pd.Series, ql: float, qh: float) -> Tuple[float, float]:\n",
    "    s_clean = s[np.isfinite(s)]\n",
    "    if s_clean.empty:\n",
    "        return (np.nan, np.nan)\n",
    "    return (s_clean.quantile(ql), s_clean.quantile(qh))\n",
    "\n",
    "\n",
    "def _winsorize_per_well(s: pd.Series, low: float, high: float) -> pd.Series:\n",
    "    if s.isna().all():\n",
    "        return s\n",
    "    ql, qh = _safe_quantiles(s, low, high)\n",
    "    if not np.isfinite(ql) or not np.isfinite(qh):\n",
    "        return s\n",
    "    return s.clip(lower=ql, upper=qh)\n",
    "\n",
    "\n",
    "def _savgol_safe(y: pd.Series, window: int, poly: int, do_interpolate: bool = True) -> pd.Series:\n",
    "    if y.isna().all():\n",
    "        return y\n",
    "    x = y.copy()\n",
    "    if do_interpolate:\n",
    "        x = x.interpolate(limit_direction=\"both\")\n",
    "    # безопасная корректировка окна\n",
    "    n = len(x)\n",
    "    w = max(3, min(window, n))\n",
    "    if w % 2 == 0:\n",
    "        w = max(3, w - 1)\n",
    "    p = min(poly, w - 1)\n",
    "    if p < 1:\n",
    "        # слишком короткий ряд — вернём как есть\n",
    "        return y if not do_interpolate else x\n",
    "    try:\n",
    "        arr = savgol_filter(x.values.astype(float), window_length=w, polyorder=p, mode=\"interp\")\n",
    "        out = pd.Series(arr, index=y.index)\n",
    "        return out\n",
    "    except Exception:\n",
    "        # на всякий случай резервация\n",
    "        return y if not do_interpolate else x\n",
    "\n",
    "\n",
    "def _month_start(ts: pd.Series) -> pd.Series:\n",
    "    # Приводим даты к началу месяца\n",
    "    return ts.dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "\n",
    "def _days_in_month(dt: pd.Timestamp) -> int:\n",
    "    # номинальные дни в месяце (если days_prod нет)\n",
    "    return (dt + pd.offsets.MonthEnd(0) - (dt - pd.offsets.MonthBegin(1))).days + 1 - 1  # безопасно\n",
    "\n",
    "\n",
    "def _weighted_rate(sum_volume: float, sum_days: float, eps: float) -> float:\n",
    "    if not np.isfinite(sum_days) or sum_days <= 0:\n",
    "        return np.nan\n",
    "    return float(sum_volume) / (float(sum_days) + eps)\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# Основная функция предобработки\n",
    "# ----------------------------------\n",
    "\n",
    "def preprocess_profiles(\n",
    "    df: pd.DataFrame,\n",
    "    cfg: Optional[PreprocConfig] = None,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Предобработка «сырых» данных до панели, выровненной по t=0 и ограниченной горизонтом T.\n",
    "    Возвращает:\n",
    "      - panel_long: long-таблица (well_name, t, date, каналы)\n",
    "      - X: np.ndarray [n_wells, T, C] — тензор по cfg.tensor_channels\n",
    "      - wells_used: список скважин, попавших в итог\n",
    "      - dropped_summary: DataFrame со статистикой отбраковки\n",
    "      - config: фактический конфиг (dict)\n",
    "\n",
    "    Требуемые столбцы: well_name, date, oil, water, gas, days_prod\n",
    "    \"\"\"\n",
    "    if cfg is None:\n",
    "        cfg = PreprocConfig()\n",
    "\n",
    "    required_cols = {\"well_name\", \"date\", \"oil\", \"water\", \"gas\", \"days_prod\"}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Отсутствуют необходимые столбцы: {missing}\")\n",
    "\n",
    "    data = df.copy()\n",
    "\n",
    "    # Типы\n",
    "    data[\"well_name\"] = data[\"well_name\"].astype(str)\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    # Фильтрация строк с невалидной датой/именем\n",
    "    data = data[ data[\"date\"].notna() & data[\"well_name\"].notna() ].copy()\n",
    "\n",
    "    # Приведение отрицательных объёмов/дней к NaN (если включено)\n",
    "    for col in [\"oil\", \"water\", \"gas\", \"days_prod\"]:\n",
    "        if col in data.columns and cfg.clamp_nonnegative:\n",
    "            data.loc[data[col] < 0, col] = np.nan\n",
    "\n",
    "    # Приведение к началу месяца\n",
    "    data[\"month\"] = _month_start(data[\"date\"])\n",
    "\n",
    "    # Если days_prod отсутствует или нулевой — используем номинальные дни месяца\n",
    "    # (это компромисс: лучше, чем делить на 30 фиксированно)\n",
    "    # Параллельно аккумулируем объёмы по месяцу\n",
    "    grp = data.groupby([\"well_name\", \"month\"], as_index=False).agg(\n",
    "        oil_sum=(\"oil\", \"sum\"),\n",
    "        water_sum=(\"water\", \"sum\"),\n",
    "        gas_sum=(\"gas\", \"sum\"),\n",
    "        days_sum=(\"days_prod\", \"sum\"),\n",
    "        valid_days=(\"days_prod\", lambda s: np.sum(np.isfinite(s) & (s > 0)))\n",
    "    )\n",
    "\n",
    "    # Фоллбек дней: если нет валидных days_prod за месяц, берём номинал\n",
    "    # (важно: если исходные числа — уже среднесуточные, эту логику нужно отключить)\n",
    "    # При необходимости пользователь может отдельно указать флаг в cfg, но для простоты — авто-детект.\n",
    "    def _fallback_days(row):\n",
    "        if row[\"valid_days\"] and row[\"valid_days\"] > 0:\n",
    "            return row[\"days_sum\"]\n",
    "        return float(_days_in_month(pd.Timestamp(row[\"month\"])))\n",
    "\n",
    "    grp[\"days_eff\"] = grp.apply(_fallback_days, axis=1)\n",
    "\n",
    "    # Среднесуточные дебиты за месяц (взвешенное по дням)\n",
    "    eps = cfg.eps\n",
    "    grp[\"r_oil\"] = grp.apply(lambda r: _weighted_rate(r[\"oil_sum\"], r[\"days_eff\"], eps), axis=1)\n",
    "    grp[\"r_water\"] = grp.apply(lambda r: _weighted_rate(r[\"water_sum\"], r[\"days_eff\"], eps), axis=1)\n",
    "    grp[\"r_gas\"] = grp.apply(lambda r: _weighted_rate(r[\"gas_sum\"], r[\"days_eff\"], eps), axis=1)\n",
    "\n",
    "    # Сортировка\n",
    "    grp = grp.sort_values([\"well_name\", \"month\"]).reset_index(drop=True)\n",
    "\n",
    "    # По-скважинная обработка: winsorize → сглаживание → производные каналы → нормализация → выравнивание/усечение\n",
    "    records: List[pd.DataFrame] = []\n",
    "    dropped: List[Tuple[str, str]] = []  # (well_name, reason)\n",
    "\n",
    "    for well, g in grp.groupby(\"well_name\", sort=False):\n",
    "        g = g.copy().reset_index(drop=True)\n",
    "\n",
    "        # Робастная подрезка выбросов по КАЖДОМУ каналу дебитов\n",
    "        for col in [\"r_oil\", \"r_water\", \"r_gas\"]:\n",
    "            g[col] = _winsorize_per_well(g[col], cfg.winsorize_low, cfg.winsorize_high)\n",
    "\n",
    "        # Интерполяция пропусков (опционально) — только для сглаживания\n",
    "        do_interp = cfg.interpolate_gaps\n",
    "\n",
    "        # Сглаживание\n",
    "        g[\"r_oil_s\"] = _savgol_safe(g[\"r_oil\"], cfg.smooth_window, cfg.smooth_poly, do_interp)\n",
    "        g[\"r_water_s\"] = _savgol_safe(g[\"r_water\"], cfg.smooth_window, cfg.smooth_poly, do_interp)\n",
    "        g[\"r_gas_s\"] = _savgol_safe(g[\"r_gas\"], cfg.smooth_window, cfg.smooth_poly, do_interp)\n",
    "\n",
    "        # Неотрицательность после сглаживания (если нужно)\n",
    "        if cfg.clamp_nonnegative:\n",
    "            for col in [\"r_oil_s\", \"r_water_s\", \"r_gas_s\"]:\n",
    "                g.loc[g[col] < 0, col] = 0.0\n",
    "\n",
    "        # Производные каналы\n",
    "        g[\"wc\"] = g[\"r_water_s\"] / (g[\"r_oil_s\"] + g[\"r_water_s\"] + eps)\n",
    "        g[\"gor\"] = g[\"r_gas_s\"] / (g[\"r_oil_s\"] + eps)\n",
    "\n",
    "        # Нормализация по пику r_oil_s\n",
    "        if cfg.normalize_by_peak:\n",
    "            peak = np.nanmax(g[\"r_oil_s\"].values) if len(g) else np.nan\n",
    "            scale = peak if (np.isfinite(peak) and peak > eps) else 1.0\n",
    "            g[\"r_oil_norm\"] = g[\"r_oil_s\"] / (scale + eps)\n",
    "            g[\"r_water_norm\"] = g[\"r_water_s\"] / (scale + eps)\n",
    "            g[\"r_gas_norm\"] = g[\"r_gas_s\"] / (scale + eps)\n",
    "        else:\n",
    "            g[\"r_oil_norm\"] = g[\"r_oil_s\"]\n",
    "            g[\"r_water_norm\"] = g[\"r_water_s\"]\n",
    "            g[\"r_gas_norm\"] = g[\"r_gas_s\"]\n",
    "\n",
    "        # Первая разность (динамика)\n",
    "        g[\"dr_oil_norm\"] = g[\"r_oil_norm\"].diff()\n",
    "\n",
    "        # Определяем точку старта t=0\n",
    "        # Условие: r_oil_s >= max(prod_threshold_abs, prod_threshold_rel * peak)\n",
    "        peak_s = np.nanmax(g[\"r_oil_s\"].values) if len(g) else np.nan\n",
    "        rel_thr = cfg.prod_threshold_rel * (peak_s if np.isfinite(peak_s) else 0.0)\n",
    "        start_thr = max(cfg.prod_threshold_abs, rel_thr)\n",
    "        start_idx = int(np.argmax(g[\"r_oil_s\"].values >= start_thr)) if np.any(g[\"r_oil_s\"].values >= start_thr) else None\n",
    "\n",
    "        if start_idx is None:\n",
    "            # Нет видимого старта — отбрасываем\n",
    "            dropped.append((well, \"no_start_detected\"))\n",
    "            continue\n",
    "\n",
    "        g = g.iloc[start_idx:].copy().reset_index(drop=True)\n",
    "\n",
    "        # Длина валидного профиля по основному каналу\n",
    "        valid_len = int(np.sum(np.isfinite(g[cfg.primary_channel])))\n",
    "        if valid_len < cfg.min_profile_months:\n",
    "            dropped.append((well, f\"too_short(<{cfg.min_profile_months})\"))\n",
    "            continue\n",
    "\n",
    "        # Усечение/паддинг до T\n",
    "        g = g.iloc[: cfg.T].copy()\n",
    "        g[\"t\"] = np.arange(len(g))\n",
    "        # Паддинг — не добавляем строки с NaN, просто оставим длину < T;\n",
    "        # при сборке тензора недостающие t будут NaN.\n",
    "\n",
    "        # Сохраняем\n",
    "        g[\"well_name\"] = well\n",
    "        records.append(g)\n",
    "\n",
    "    if not records:\n",
    "        panel_long = pd.DataFrame(columns=[\n",
    "            \"well_name\", \"t\", \"month\",\n",
    "            \"r_oil\", \"r_water\", \"r_gas\",\n",
    "            \"r_oil_s\", \"r_water_s\", \"r_gas_s\",\n",
    "            \"wc\", \"gor\",\n",
    "            \"r_oil_norm\", \"r_water_norm\", \"r_gas_norm\",\n",
    "            \"dr_oil_norm\"\n",
    "        ])\n",
    "        X = np.empty((0, cfg.T, len(cfg.tensor_channels)), dtype=float)\n",
    "        wells_used: List[str] = []\n",
    "    else:\n",
    "        panel_long = pd.concat(records, ignore_index=True)\n",
    "        panel_long = panel_long.rename(columns={\"month\": \"date\"})\n",
    "\n",
    "        # Сборка тензора [n_wells, T, C] по cfg.tensor_channels\n",
    "        channels = list(cfg.tensor_channels)\n",
    "        wells_used = panel_long[\"well_name\"].unique().tolist()\n",
    "        n = len(wells_used)\n",
    "        C = len(channels)\n",
    "        T = cfg.T\n",
    "        X = np.full((n, T, C), np.nan, dtype=float)\n",
    "\n",
    "        # Быстрый доступ к индексам\n",
    "        well_to_idx = {w: i for i, w in enumerate(wells_used)}\n",
    "        ch_to_idx = {ch: j for j, ch in enumerate(channels)}\n",
    "\n",
    "        for w, g in panel_long.groupby(\"well_name\", sort=False):\n",
    "            i = well_to_idx[w]\n",
    "            # гарантируем t в пределах [0, T-1]\n",
    "            g = g[(g[\"t\"] >= 0) & (g[\"t\"] < T)]\n",
    "            for ch in channels:\n",
    "                if ch not in g.columns:\n",
    "                    continue\n",
    "                X[i, g[\"t\"].astype(int).values, ch_to_idx[ch]] = g[ch].values\n",
    "\n",
    "    # Сводка по отбросам\n",
    "    if dropped:\n",
    "        dropped_df = pd.DataFrame(dropped, columns=[\"well_name\", \"reason\"])\n",
    "        dropped_summary = dropped_df[\"reason\"].value_counts().rename_axis(\"reason\").reset_index(name=\"count\")\n",
    "    else:\n",
    "        dropped_summary = pd.DataFrame(columns=[\"reason\", \"count\"])\n",
    "\n",
    "    result = {\n",
    "        \"panel_long\": panel_long,\n",
    "        \"X\": X,\n",
    "        \"wells_used\": wells_used,\n",
    "        \"dropped_summary\": dropped_summary,\n",
    "        \"config\": asdict(cfg),\n",
    "        \"tensor_channels\": list(cfg.tensor_channels),\n",
    "    }\n",
    "    print(\"Preprocess complete.\")\n",
    "    print(f\"  В итог попало скважин: {len(result['wells_used'])}\")\n",
    "    if len(dropped_summary):\n",
    "        print(\"  Отброшено (по причинам):\")\n",
    "        display(dropped_summary)\n",
    "    else:\n",
    "        print(\"  Отброшенных скважин нет.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Пример использования\n",
    "# ----------------------------\n",
    "if 'df' in globals():\n",
    "    print(\"Обнаружен df в окружении. Запускаю предобработку с дефолтным конфигом...\")\n",
    "    cfg = PreprocConfig()\n",
    "    out = preprocess_profiles(df, cfg)\n",
    "    # Покажем заголовок long-таблицы как предварительный просмотр\n",
    "    head_preview = out[\"panel_long\"].head(12).copy()\n",
    "    try:\n",
    "        # Если доступна вспомогательная функция отображения таблиц — используем её\n",
    "        import caas_jupyter_tools\n",
    "        caas_jupyter_tools.display_dataframe_to_user(\"Предпросмотр panel_long\", head_preview)\n",
    "    except Exception:\n",
    "        # Иначе просто печатаем\n",
    "        print(head_preview)\n",
    "else:\n",
    "    print(\"Готово. Функции предобработки загружены. Для запуска вызовите:\")\n",
    "    print(\"cfg = PreprocConfig(T=36, min_profile_months=12)\")\n",
    "    print(\"out = preprocess_profiles(df, cfg)\")\n",
    "    print(\"panel_long, X = out['panel_long'], out['X']\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обнаружен df в окружении. Запускаю предобработку с дефолтным конфигом...\n",
      "Preprocess complete.\n",
      "  В итог попало скважин: 2142\n",
      "  Отброшено (по причинам):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "              reason  count\n",
       "0  no_start_detected   1930\n",
       "1     too_short(<12)     51"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reason</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_start_detected</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>too_short(&lt;12)</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             well_name       date  oil_sum  water_sum  gas_sum  days_sum  \\\n",
      "0   25-003-05007-00-00 1986-01-01    252.0     2552.0      0.0      31.0   \n",
      "1   25-003-05007-00-00 1986-02-01    239.0     2295.0      0.0      28.0   \n",
      "2   25-003-05007-00-00 1986-03-01    253.0     2697.0      0.0      31.0   \n",
      "3   25-003-05007-00-00 1986-04-01    249.0     1883.0      0.0      30.0   \n",
      "4   25-003-05007-00-00 1986-05-01    278.0     2631.0      0.0      31.0   \n",
      "5   25-003-05007-00-00 1986-06-01    244.0     2086.0      0.0      30.0   \n",
      "6   25-003-05007-00-00 1986-07-01    271.0     2322.0      0.0      31.0   \n",
      "7   25-003-05007-00-00 1986-09-01    195.0     1625.0      0.0       0.0   \n",
      "8   25-003-05007-00-00 1986-10-01    224.0     1867.0      0.0       0.0   \n",
      "9   25-003-05007-00-00 1986-11-01    209.0     1742.0      0.0       0.0   \n",
      "10  25-003-05007-00-00 1986-12-01    224.0     1867.0      0.0       0.0   \n",
      "11  25-003-05007-00-00 1987-01-01    221.0     2066.0      0.0       0.0   \n",
      "\n",
      "    valid_days  days_eff     r_oil    r_water  ...   r_oil_s  r_water_s  \\\n",
      "0            1      31.0  8.129032  82.322581  ...  8.278308  84.841661   \n",
      "1            1      28.0  8.535714  81.964286  ...  8.200639  80.076689   \n",
      "2            1      31.0  8.161290  87.000000  ...  8.270862  77.548308   \n",
      "3            1      30.0  8.300000  62.766667  ...  8.475464  76.428345   \n",
      "4            1      31.0  8.967742  84.870968  ...  8.541198  72.705622   \n",
      "5            1      30.0  8.133333  69.533333  ...  9.032366  80.851628   \n",
      "6            1      31.0  8.741935  74.903226  ...  7.060276  59.565484   \n",
      "7            0      60.0  3.250000  27.083333  ...  4.860092  41.055868   \n",
      "8            0      60.0  3.733333  31.116667  ...  3.052596  25.266390   \n",
      "9            0      60.0  3.483333  29.033333  ...  3.662795  30.214575   \n",
      "10           0      60.0  3.733333  31.116667  ...  3.632729  30.933327   \n",
      "11           0      61.0  3.622951  33.868852  ...  3.616024  33.661620   \n",
      "\n",
      "    r_gas_s        wc  gor  r_oil_norm  r_water_norm  r_gas_norm  dr_oil_norm  \\\n",
      "0       0.0  0.911101  0.0    0.754343      7.731016         0.0          NaN   \n",
      "1       0.0  0.907104  0.0    0.747266      7.296818         0.0    -0.007077   \n",
      "2       0.0  0.903625  0.0    0.753665      7.066424         0.0     0.006399   \n",
      "3       0.0  0.900176  0.0    0.772309      6.964370         0.0     0.018644   \n",
      "4       0.0  0.894873  0.0    0.778298      6.625145         0.0     0.005990   \n",
      "5       0.0  0.899511  0.0    0.823055      7.367432         0.0     0.044757   \n",
      "6       0.0  0.894031  0.0    0.643353      5.427778         0.0    -0.179702   \n",
      "7       0.0  0.894152  0.0    0.442866      3.741128         0.0    -0.200487   \n",
      "8       0.0  0.892207  0.0    0.278161      2.302346         0.0    -0.164704   \n",
      "9       0.0  0.891881  0.0    0.333764      2.753239         0.0     0.055603   \n",
      "10      0.0  0.894905  0.0    0.331025      2.818734         0.0    -0.002740   \n",
      "11      0.0  0.902998  0.0    0.329503      3.067343         0.0    -0.001522   \n",
      "\n",
      "     t  \n",
      "0    0  \n",
      "1    1  \n",
      "2    2  \n",
      "3    3  \n",
      "4    4  \n",
      "5    5  \n",
      "6    6  \n",
      "7    7  \n",
      "8    8  \n",
      "9    9  \n",
      "10  10  \n",
      "11  11  \n",
      "\n",
      "[12 rows x 21 columns]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "d5b5d8c8ce36eb77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:47:10.856674Z",
     "start_time": "2025-09-14T11:34:08.802765Z"
    }
   },
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Production Behavior Manifold (PBM) — Шаг 2 (признаки) и Шаг 3 (Manifold)\n",
    "-----------------------------------------------------------------------\n",
    "Зависимости (pip): numpy, pandas, scikit-learn, umap-learn, fastdtw, tqdm (опц.)\n",
    "\n",
    "Ожидается, что из Шага 1 у вас уже есть объект `out` с ключами:\n",
    "  - out[\"panel_long\"]: long-таблица по скважинам (после предобработки)\n",
    "  - out[\"X\"]: np.ndarray формы [n_wells, T, C] по каналам cfg.tensor_channels\n",
    "  - out[\"wells_used\"]: список имён скважин в том же порядке, что и ось 0 в X\n",
    "  - out[\"tensor_channels\"]: список имён каналов в X (например: [\"r_oil_norm\",\"wc\",\"gor\",\"dr_oil_norm\"]) \n",
    "\n",
    "Этот файл добавляет:\n",
    "  1) Шаг 2: компактные дескрипторы профилей (side features)\n",
    "  2) Шаг 3: построение низкоразмерной \"карты поведения\" (UMAP) двумя способами:\n",
    "       a) быстрый базовый (евклид на выровненных рядах)\n",
    "       b) уточнённый с FastDTW, без полной O(N^2) — через доуточнение k ближайших пар\n",
    "\n",
    "Примечание: библиотеку dtaidistance *не* используем. Для DTW берём fastdtw.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, List, Optional, Sequence, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# UMAP\n",
    "try:\n",
    "    import umap\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Требуется пакет 'umap-learn'. Установите: pip install umap-learn\")\n",
    "\n",
    "# fastdtw\n",
    "try:\n",
    "    from fastdtw import fastdtw\n",
    "except Exception:\n",
    "    fastdtw = None\n",
    "    warnings.warn(\"fastdtw не найден. DTW-вариант будет недоступен, останется базовый евклид.\")\n",
    "\n",
    "# tqdm (опционально)\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    _TQDM = True\n",
    "except Exception:\n",
    "    _TQDM = False\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# -------------------- ШАГ 2: ФИЧИ ---------------------\n",
    "# ======================================================\n",
    "\n",
    "def _nanpolyfit(y: np.ndarray, x: Optional[np.ndarray] = None, deg: int = 1) -> Tuple[float, ...]:\n",
    "    \"\"\"Лин. регрессия по ряду с NaN. Возвращает коэффициенты (последний — свободный член).\n",
    "    deg=1 => (slope, intercept).\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, float)\n",
    "    if x is None:\n",
    "        x = np.arange(len(y), dtype=float)\n",
    "    mask = np.isfinite(y) & np.isfinite(x)\n",
    "    if mask.sum() < deg + 1:\n",
    "        return tuple([np.nan] * (deg + 1))\n",
    "    coeffs = np.polyfit(x[mask], y[mask], deg=deg)\n",
    "    return tuple(coeffs)\n",
    "\n",
    "\n",
    "def _first_index_where(y: np.ndarray, cond) -> Optional[int]:\n",
    "    idx = np.nonzero(cond(y))[0]\n",
    "    return int(idx[0]) if idx.size else None\n",
    "\n",
    "\n",
    "def _rolling_stat(y: np.ndarray, win: int, fn) -> float:\n",
    "    \"\"\"Одно число: fn по всем скользящим окнам длины win, агрегация = среднее.\n",
    "    Например, fn = np.std или пользовательская функция.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, float)\n",
    "    if win <= 0 or len(y) < win:\n",
    "        return np.nan\n",
    "    vals = []\n",
    "    for i in range(len(y) - win + 1):\n",
    "        seg = y[i:i+win]\n",
    "        if np.isfinite(seg).sum() < max(2, win // 2):\n",
    "            continue\n",
    "        vals.append(fn(seg[np.isfinite(seg)]))\n",
    "    return float(np.mean(vals)) if len(vals) else np.nan\n",
    "\n",
    "\n",
    "def _slope_over_window(y: np.ndarray, win: int) -> float:\n",
    "    if win <= 1 or len(y) < win:\n",
    "        return np.nan\n",
    "    slopes = []\n",
    "    for i in range(len(y) - win + 1):\n",
    "        seg = y[i:i+win]\n",
    "        if np.isfinite(seg).sum() < max(2, win // 2):\n",
    "            continue\n",
    "        a, b = _nanpolyfit(seg, np.arange(win), deg=1)\n",
    "        slopes.append(a)\n",
    "    return float(np.mean(slopes)) if slopes else np.nan\n",
    "\n",
    "\n",
    "def compute_side_features(panel_long: pd.DataFrame, T: int = 36) -> pd.DataFrame:\n",
    "    \"\"\"Строит компактные дескрипторы профиля по каждой скважине на горизонте [0..T).\n",
    "\n",
    "    Требуемые колонки в panel_long: [well_name, t, r_oil_s, r_oil_norm, wc, gor, dr_oil_norm]\n",
    "    Возвращает DataFrame: по одной строке на скважину.\n",
    "    \"\"\"\n",
    "    need = {\"well_name\", \"t\", \"r_oil_s\", \"r_oil_norm\", \"wc\", \"gor\", \"dr_oil_norm\"}\n",
    "    missing = need - set(panel_long.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"compute_side_features: не хватает колонок: {missing}\")\n",
    "\n",
    "    feats: List[pd.DataFrame] = []\n",
    "    for w, g in panel_long.groupby(\"well_name\", sort=False):\n",
    "        g = g.copy()\n",
    "        g = g[g[\"t\"].between(0, T-1)]\n",
    "        g = g.sort_values(\"t\")\n",
    "        # Векторы (длина может быть < T)\n",
    "        oil = g[\"r_oil_s\"].to_numpy(float)\n",
    "        oil_norm = g[\"r_oil_norm\"].to_numpy(float)\n",
    "        wc = g[\"wc\"].to_numpy(float)\n",
    "        gor = g[\"gor\"].to_numpy(float)\n",
    "        d_oil = g[\"dr_oil_norm\"].to_numpy(float)\n",
    "\n",
    "        # Базовые величины\n",
    "        peak = np.nanmax(oil) if oil.size else np.nan\n",
    "        t_peak = int(np.nanargmax(oil)) if (oil.size and np.isfinite(peak)) else np.nan\n",
    "\n",
    "        # Время до 50% и 20% от пика (decline speed)\n",
    "        half = 0.5 * peak if np.isfinite(peak) else np.nan\n",
    "        fifth = 0.2 * peak if np.isfinite(peak) else np.nan\n",
    "        t_half = _first_index_where(oil, lambda y: np.isfinite(half) and (y <= half))\n",
    "        t_20 = _first_index_where(oil, lambda y: np.isfinite(fifth) and (y <= fifth))\n",
    "\n",
    "        # Усреднения по ранним/средним окнам\n",
    "        m6 = int(min(6, len(oil)))\n",
    "        m12 = int(min(12, len(oil)))\n",
    "        early_mean6 = float(np.nanmean(oil_norm[:m6])) if m6 else np.nan\n",
    "        early_mean12 = float(np.nanmean(oil_norm[:m12])) if m12 else np.nan\n",
    "        early_trend6, _ = _nanpolyfit(oil_norm[:m6], np.arange(m6), 1) if m6 >= 3 else (np.nan, np.nan)\n",
    "        mid_trend12 = _slope_over_window(oil_norm, 12)\n",
    "\n",
    "        # Всплески/скачкообразность\n",
    "        vol_doil_w6 = _rolling_stat(d_oil, win=6, fn=np.std)\n",
    "        vol_doil_w12 = _rolling_stat(d_oil, win=12, fn=np.std)\n",
    "\n",
    "        # Водоотдача и газовый фактор — уровни и тренды\n",
    "        wc_mean12 = float(np.nanmean(wc[:m12])) if m12 else np.nan\n",
    "        wc_trend12 = _slope_over_window(wc, 12)\n",
    "        gor_trend12 = _slope_over_window(gor, 12)\n",
    "\n",
    "        # Плато: сколько месяцев oil_norm >= 0.9 в начале профиля\n",
    "        plateau_len = 0\n",
    "        for v in oil_norm:\n",
    "            if not np.isfinite(v) or v < 0.9:\n",
    "                break\n",
    "            plateau_len += 1\n",
    "\n",
    "        # Системные пропуски\n",
    "        valid_ratio = float(np.isfinite(oil).sum() / max(1, T))\n",
    "\n",
    "        row = pd.DataFrame({\n",
    "            \"well_name\": [w],\n",
    "            \"peak_oil\": [peak],\n",
    "            \"t_peak\": [t_peak],\n",
    "            \"t_half\": [t_half],\n",
    "            \"t_20\": [t_20],\n",
    "            \"early_mean6\": [early_mean6],\n",
    "            \"early_mean12\": [early_mean12],\n",
    "            \"early_trend6\": [early_trend6],\n",
    "            \"mid_trend12\": [mid_trend12],\n",
    "            \"vol_doil_w6\": [vol_doil_w6],\n",
    "            \"vol_doil_w12\": [vol_doil_w12],\n",
    "            \"wc_mean12\": [wc_mean12],\n",
    "            \"wc_trend12\": [wc_trend12],\n",
    "            \"gor_trend12\": [gor_trend12],\n",
    "            \"plateau_len\": [plateau_len],\n",
    "            \"valid_ratio\": [valid_ratio],\n",
    "        })\n",
    "        feats.append(row)\n",
    "\n",
    "    feats_df = pd.concat(feats, ignore_index=True) if feats else pd.DataFrame()\n",
    "    return feats_df\n",
    "\n",
    "\n",
    "def scale_features(feats_df: pd.DataFrame, exclude: Sequence[str] = (\"well_name\",)) -> Tuple[pd.DataFrame, RobustScaler]:\n",
    "    if feats_df.empty:\n",
    "        return feats_df, RobustScaler()\n",
    "    cols = [c for c in feats_df.columns if c not in exclude]\n",
    "    scaler = RobustScaler()\n",
    "    X = scaler.fit_transform(feats_df[cols].astype(float).values)\n",
    "    feats_scaled = feats_df.copy()\n",
    "    for i, c in enumerate(cols):\n",
    "        feats_scaled[c] = X[:, i]\n",
    "    return feats_scaled, scaler\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# --------------- ШАГ 3: MANIFOLD (UMAP) ---------------\n",
    "# ======================================================\n",
    "\n",
    "@dataclass\n",
    "class ManifoldConfig:\n",
    "    # Какие каналы из тензора X брать для \"формы\"\n",
    "    channels: Tuple[str, ...] = (\"r_oil_norm\", \"wc\")\n",
    "    # DTW\n",
    "    fastdtw_radius: int = 6\n",
    "    k_refine: int = 40            # сколько ближайших по евклиду пар доуточнять DTW\n",
    "    weights: Optional[Tuple[float, ...]] = (0.7, 0.3)  # веса каналов в многомерной метрике\n",
    "    # UMAP\n",
    "    n_neighbors: int = 30\n",
    "    min_dist: float = 0.05\n",
    "    n_components: int = 2\n",
    "    random_state: int = 43\n",
    "\n",
    "\n",
    "def _flatten_series_matrix(X: np.ndarray, channels_idx: Sequence[int]) -> np.ndarray:\n",
    "    \"\"\"Преобразует [N, T, C] -> [N, T*len(channels_idx)], NaN -> 0.\n",
    "    Этот вектор используется ТОЛЬКО для быстрой евклидовой близости/UMAP-базы.\n",
    "    \"\"\"\n",
    "    x = X[:, :, channels_idx]  # [N,T,C']\n",
    "    N, T, C = x.shape\n",
    "    x = np.transpose(x, (0, 2, 1)).reshape(N, C * T)\n",
    "    x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return x\n",
    "\n",
    "\n",
    "def embed_umap_euclid(X: np.ndarray, tensor_channels: Sequence[str], channels: Sequence[str],\n",
    "                      n_neighbors: int = 30, min_dist: float = 0.05, n_components: int = 2,\n",
    "                      random_state: int = 42) -> Tuple[np.ndarray, umap.UMAP]:\n",
    "    \"\"\"Быстрый базовый manifold: UMAP по евклиду на выровненных рядах.\n",
    "    Возвращает (Z, umap_model).\n",
    "    \"\"\"\n",
    "    ch_to_idx = {c: i for i, c in enumerate(tensor_channels)}\n",
    "    idx = [ch_to_idx[c] for c in channels if c in ch_to_idx]\n",
    "    X2 = _flatten_series_matrix(X, idx)\n",
    "    model = umap.UMAP(\n",
    "        n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,\n",
    "        metric=\"euclidean\", random_state=random_state\n",
    "    )\n",
    "    Z = model.fit_transform(X2)\n",
    "    return Z, model\n",
    "\n",
    "\n",
    "def _fastdtw_multichannel(A: np.ndarray, B: np.ndarray, weights: Optional[Sequence[float]] = None, radius: int = 6) -> float:\n",
    "    \"\"\"DTW для многоканальных рядов формы [T,C]. Использует L2 по каналам с весами.\n",
    "    Отбрасывает позиции, где все каналы NaN. Заполняет оставшиеся NaN нулями.\n",
    "    \"\"\"\n",
    "    if fastdtw is None:\n",
    "        raise RuntimeError(\"fastdtw не установлен\")\n",
    "    A = np.asarray(A, float)\n",
    "    B = np.asarray(B, float)\n",
    "    if A.ndim != 2 or B.ndim != 2:\n",
    "        raise ValueError(\"Ожидается [T,C] для обоих рядов\")\n",
    "    # маска валидных позиций (хотя бы один канал не NaN)\n",
    "    maskA = np.any(np.isfinite(A), axis=1)\n",
    "    maskB = np.any(np.isfinite(B), axis=1)\n",
    "    A = A[maskA]\n",
    "    B = B[maskB]\n",
    "    if A.size == 0 or B.size == 0:\n",
    "        return float(\"inf\")\n",
    "    # NaN -> 0 (после выкидывания полностью пустых кадров)\n",
    "    A = np.nan_to_num(A, nan=0.0)\n",
    "    B = np.nan_to_num(B, nan=0.0)\n",
    "    # взвешивание каналов\n",
    "    if weights is not None:\n",
    "        w = np.asarray(weights, float).reshape(1, -1)\n",
    "        if w.shape[1] != A.shape[1]:\n",
    "            raise ValueError(\"Длина weights должна совпадать с числом каналов\")\n",
    "        A = A * w\n",
    "        B = B * w\n",
    "    # метрика между точками — евклид в C-мерном пространстве\n",
    "    dist, _ = fastdtw(A, B, radius=radius, dist=2)\n",
    "    return float(dist)\n",
    "\n",
    "\n",
    "def embed_umap_fastdtw(\n",
    "    X: np.ndarray,\n",
    "    tensor_channels: Sequence[str],\n",
    "    channels: Sequence[str],\n",
    "    cfg: Optional[ManifoldConfig] = None,\n",
    "    sample_size: Optional[int] = None,\n",
    "    candidate_knn: Optional[int] = None,\n",
    ") -> Tuple[np.ndarray, List[str], np.ndarray, Dict[str, object]]:\n",
    "    \"\"\"UMAP с уточнением FastDTW для ближайших пар.\n",
    "\n",
    "    Алгоритм:\n",
    "      1) Строим евклидову матрицу расстояний E в пространстве выровненных рядов (быстро).\n",
    "      2) Для каждой точки берём k_refine ближайших по E и пересчитываем расстояния DTW (fastdtw).\n",
    "      3) Получаем полную матрицу D: для уточнённых пар D=DTW, иначе D=E (масштаб сопоставим, но разный — это ок для UMAP).\n",
    "      4) UMAP(metric='precomputed') на D.\n",
    "\n",
    "    Если sample_size задан, берём случайную подвыборку для ускорения.\n",
    "\n",
    "    Возвращает: (Z, wells_sub, D, info)\n",
    "    \"\"\"\n",
    "    if cfg is None:\n",
    "        cfg = ManifoldConfig()\n",
    "\n",
    "    rng = np.random.default_rng(cfg.random_state)\n",
    "    N, T, Ctot = X.shape\n",
    "    ch_to_idx = {c: i for i, c in enumerate(tensor_channels)}\n",
    "    idx = [ch_to_idx[c] for c in channels if c in ch_to_idx]\n",
    "    if len(idx) != len(channels):\n",
    "        missing = [c for c in channels if c not in ch_to_idx]\n",
    "        raise ValueError(f\"В тензоре X нет каналов: {missing}\")\n",
    "\n",
    "    # Подвыборка\n",
    "    all_idx = np.arange(N)\n",
    "    if sample_size is not None and sample_size < N:\n",
    "        sub_idx = rng.choice(all_idx, size=sample_size, replace=False)\n",
    "    else:\n",
    "        sub_idx = all_idx\n",
    "    Xs = X[sub_idx][:, :, idx]  # [Ns, T, C']\n",
    "\n",
    "    # База: евклид на флеттене\n",
    "    Xflat = _flatten_series_matrix(Xs, list(range(len(idx))))  # [Ns, T*C']\n",
    "    E = pairwise_distances(Xflat, metric=\"euclidean\")  # [Ns,Ns]\n",
    "\n",
    "    Ns = Xs.shape[0]\n",
    "    D = E.copy()\n",
    "\n",
    "    if fastdtw is None:\n",
    "        warnings.warn(\"fastdtw недоступен — возвращаю UMAP по евклиду (без уточнения DTW)\")\n",
    "    else:\n",
    "        # Список пар для уточнения — top-k по E для каждой точки\n",
    "        k = cfg.k_refine if candidate_knn is None else candidate_knn\n",
    "        nn = NearestNeighbors(n_neighbors=min(k+1, Ns), metric=\"euclidean\")\n",
    "        nn.fit(Xflat)\n",
    "        dists, inds = nn.kneighbors(Xflat, return_distance=True)\n",
    "        # inds[:,0] — сама точка; начнём с 1..\n",
    "        pairs = set()\n",
    "        for i in range(Ns):\n",
    "            for j in inds[i, 1:]:\n",
    "                a, b = (int(i), int(j)) if i < j else (int(j), int(i))\n",
    "                if a != b:\n",
    "                    pairs.add((a, b))\n",
    "        pairs = list(pairs)\n",
    "\n",
    "        it = pairs\n",
    "        if _TQDM:\n",
    "            it = tqdm(pairs, desc=f\"Recompute DTW for {len(pairs)} pairs (radius={cfg.fastdtw_radius})\")\n",
    "\n",
    "        for (i, j) in it:\n",
    "            d = _fastdtw_multichannel(Xs[i], Xs[j], weights=cfg.weights, radius=cfg.fastdtw_radius)\n",
    "            if not math.isfinite(d):\n",
    "                continue\n",
    "            D[i, j] = d\n",
    "            D[j, i] = d\n",
    "\n",
    "    # UMAP на предвычисленной матрице\n",
    "    model = umap.UMAP(\n",
    "        n_neighbors=cfg.n_neighbors,\n",
    "        min_dist=cfg.min_dist,\n",
    "        n_components=cfg.n_components,\n",
    "        metric=\"precomputed\",\n",
    "        random_state=cfg.random_state,\n",
    "    )\n",
    "    Z = model.fit_transform(D)\n",
    "\n",
    "    info = {\n",
    "        \"Ns\": int(Ns),\n",
    "        \"channels\": list(channels),\n",
    "        \"k_refine\": int(cfg.k_refine),\n",
    "        \"fastdtw_radius\": int(cfg.fastdtw_radius),\n",
    "        \"umap_params\": {\n",
    "            \"n_neighbors\": cfg.n_neighbors,\n",
    "            \"min_dist\": cfg.min_dist,\n",
    "            \"n_components\": cfg.n_components,\n",
    "            \"random_state\": cfg.random_state,\n",
    "        }\n",
    "    }\n",
    "    return Z, sub_idx.tolist(), D, info\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# ======================================================\n",
    "\n",
    "if 'out' in globals():\n",
    "    panel_long = out[\"panel_long\"]\n",
    "    X = out[\"X\"]\n",
    "    wells = out[\"wells_used\"]\n",
    "    tensor_channels = out[\"tensor_channels\"]\n",
    "    T = out[\"config\"][\"T\"] if \"config\" in out else X.shape[1]\n",
    "\n",
    "    # --- Шаг 2: компактные фичи\n",
    "    feats = compute_side_features(panel_long, T=T)\n",
    "    feats_scaled, scaler = scale_features(feats)\n",
    "    print(\"Side features shape:\", feats_scaled.shape)\n",
    "\n",
    "    # --- Шаг 3a: быстрый UMAP по евклиду на рядах\n",
    "    Z_euclid, umap_e = embed_umap_euclid(\n",
    "        X, tensor_channels=tensor_channels, channels=[\"r_oil_norm\", \"wc\"],\n",
    "        n_neighbors=30, min_dist=0.05, n_components=2, random_state=42\n",
    "    )\n",
    "\n",
    "    # --- Шаг 3b: UMAP с FastDTW-уточнением\n",
    "    cfg_m = ManifoldConfig(channels=(\"r_oil_norm\", \"wc\"), fastdtw_radius=6, k_refine=40,\n",
    "                           weights=(0.7, 0.3), n_neighbors=30, min_dist=0.05,\n",
    "                           n_components=2, random_state=42)\n",
    "    Z_dtw, sub_idx, D, info = embed_umap_fastdtw(\n",
    "        X, tensor_channels=tensor_channels, channels=cfg_m.channels,\n",
    "        cfg=cfg_m, sample_size=500  # можно None для полного набора\n",
    "    )\n",
    "    print(\"DTW-UMAP:\", Z_dtw.shape, \"subset size:\", len(sub_idx))\n",
    "\n",
    "    # Пример: собрать DataFrame с координатами\n",
    "    df_map = pd.DataFrame({\n",
    "        \"well_name\": np.array(wells)[sub_idx],\n",
    "        \"x\": Z_dtw[:,0],\n",
    "        \"y\": Z_dtw[:,1],\n",
    "    })\n",
    "    display(df_map.head())\n",
    "else:\n",
    "    print(\"Для примера использования требуется объект 'out' из Шага 1.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Side features shape: (2142, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recompute DTW for 64583 pairs (radius=6): 100%|██████████| 64583/64583 [12:43<00:00, 84.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTW-UMAP: (2142, 2) subset size: 2142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "            well_name         x          y\n",
       "0  25-003-05007-00-00  7.190036   9.426781\n",
       "1  25-003-05009-00-00  7.079275  10.082273\n",
       "2  25-003-05057-00-00  7.511279  10.024635\n",
       "3  25-003-05067-00-00  6.941536   9.197872\n",
       "4  25-003-05068-00-00  7.686371  10.157837"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>well_name</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25-003-05007-00-00</td>\n",
       "      <td>7.190036</td>\n",
       "      <td>9.426781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25-003-05009-00-00</td>\n",
       "      <td>7.079275</td>\n",
       "      <td>10.082273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25-003-05057-00-00</td>\n",
       "      <td>7.511279</td>\n",
       "      <td>10.024635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25-003-05067-00-00</td>\n",
       "      <td>6.941536</td>\n",
       "      <td>9.197872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25-003-05068-00-00</td>\n",
       "      <td>7.686371</td>\n",
       "      <td>10.157837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "15c5ff6b77e71a3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:48:25.451562Z",
     "start_time": "2025-09-14T11:47:10.960043Z"
    }
   },
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Production Behavior Manifold (PBM) — Шаг 4: Сегментация и аномалии\n",
    "------------------------------------------------------------------\n",
    "Зависимости (pip): numpy, pandas, scikit-learn, hdbscan, (опц.) tslearn\n",
    "\n",
    "Входы из предыдущих шагов:\n",
    "  - out: результат preprocess_profiles (Шаг 1)\n",
    "      * out[\"panel_long\"] — long-таблица после выравнивания t=0\n",
    "      * out[\"X\"] — тензор [N, T, C]\n",
    "      * out[\"wells_used\"] — список имён в порядке X\n",
    "      * out[\"tensor_channels\"] — имена каналов в X\n",
    "      * out[\"config\"][\"T\"] — горизонт T\n",
    "  - Z, sub_idx: embedding и индексы скважин, вошедшие в него (из Шага 3)\n",
    "\n",
    "Функционал:\n",
    "  1) Кластеризация HDBSCAN на manifold (поддержка шума/аномалий)\n",
    "  2) Альтернатива: GMM по BIC\n",
    "  3) Аномалии: LOF + расстояние до медоида кластера\n",
    "  4) Прототипы кластеров (медианные профили и, опционально, soft-DTW/DTW барицентры)\n",
    "  5) Метрики качества: Silhouette (без шума), DBCV (для HDBSCAN)\n",
    "  6) Удобные хелперы для сборки отчёта\n",
    "\n",
    "Примечание: dtaidistance НЕ используется. Для soft-DTW/DBA — опционально tslearn.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# --- HDBSCAN ---\n",
    "try:\n",
    "    import hdbscan\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Требуется пакет 'hdbscan'. Установите: pip install hdbscan\")\n",
    "\n",
    "# --- tslearn (опционально для барицентров) ---\n",
    "try:\n",
    "    from tslearn.barycenters import softdtw_barycenter, dtw_barycenter_averaging\n",
    "    _TSLEARN_OK = True\n",
    "except Exception:\n",
    "    _TSLEARN_OK = False\n",
    "    warnings.warn(\"tslearn не найден: прототипы будут как медианные кривые (без барицентров)\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# ---------------- КЛАСТЕРИЗАЦИЯ -----------------------\n",
    "# ======================================================\n",
    "\n",
    "@dataclass\n",
    "class ClusterConfig:\n",
    "    # HDBSCAN\n",
    "    min_cluster_size: int = 50\n",
    "    min_samples: int = 12\n",
    "    cluster_selection_epsilon: float = 0.0\n",
    "    allow_single_cluster: bool = False\n",
    "    # LOF\n",
    "    lof_neighbors: int = 30\n",
    "\n",
    "\n",
    "def cluster_hdbscan(\n",
    "    Z: np.ndarray,\n",
    "    wells_sub: Sequence[str],\n",
    "    cfg: Optional[ClusterConfig] = None,\n",
    "    metric: str = \"euclidean\",\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"Кластеризация HDBSCAN на низкоразмерных координатах Z.\n",
    "\n",
    "    Возвращает dict:\n",
    "      labels: np.array [Ns]\n",
    "      probabilities: np.array [Ns]\n",
    "      clusterer: HDBSCAN объект\n",
    "      df_map: DataFrame(well_name, x, y, cluster, prob)\n",
    "      silhouette: float | np.nan (по точкам с cluster>=0)\n",
    "      dbcv: float | np.nan\n",
    "    \"\"\"\n",
    "    if cfg is None:\n",
    "        cfg = ClusterConfig()\n",
    "\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=cfg.min_cluster_size,\n",
    "        min_samples=cfg.min_samples,\n",
    "        metric=metric,\n",
    "        cluster_selection_epsilon=cfg.cluster_selection_epsilon,\n",
    "        allow_single_cluster=cfg.allow_single_cluster,\n",
    "        prediction_data=True,\n",
    "    )\n",
    "    labels = clusterer.fit_predict(Z)\n",
    "    probs = getattr(clusterer, \"probabilities_\", np.ones(len(labels)))\n",
    "\n",
    "    # Карта\n",
    "    df_map = pd.DataFrame({\n",
    "        \"well_name\": wells_sub,\n",
    "        \"x\": Z[:, 0],\n",
    "        \"y\": Z[:, 1],\n",
    "        \"cluster\": labels,\n",
    "        \"prob\": probs,\n",
    "    })\n",
    "\n",
    "    # Метрики\n",
    "    mask = labels >= 0\n",
    "    if mask.sum() > 1 and len(np.unique(labels[mask])) > 1:\n",
    "        try:\n",
    "            sil = float(silhouette_score(Z[mask], labels[mask], metric=\"euclidean\"))\n",
    "        except Exception:\n",
    "            sil = float(\"nan\")\n",
    "    else:\n",
    "        sil = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        dbcv = float(hdbscan.validity.validity_index(Z, labels, metric=\"euclidean\"))\n",
    "    except Exception:\n",
    "        dbcv = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"labels\": labels,\n",
    "        \"probabilities\": probs,\n",
    "        \"clusterer\": clusterer,\n",
    "        \"df_map\": df_map,\n",
    "        \"silhouette\": sil,\n",
    "        \"dbcv\": dbcv,\n",
    "    }\n",
    "\n",
    "\n",
    "def cluster_gmm_bic(Z: np.ndarray, wells_sub: Sequence[str], n_range: Sequence[int] = range(2, 11)) -> Dict[str, object]:\n",
    "    \"\"\"Альтернативная кластеризация GMM с выбором числа компонент по BIC.\n",
    "    Возвращает dict аналогичный HDBSCAN (labels, df_map, silhouette, model, best_k, bic_table).\n",
    "    \"\"\"\n",
    "    best_bic = np.inf\n",
    "    best = None\n",
    "    rows = []\n",
    "    for k in n_range:\n",
    "        gm = GaussianMixture(n_components=k, covariance_type=\"full\", random_state=42)\n",
    "        gm.fit(Z)\n",
    "        bic = gm.bic(Z)\n",
    "        rows.append({\"k\": k, \"bic\": bic})\n",
    "        if bic < best_bic:\n",
    "            best_bic = bic\n",
    "            best = gm\n",
    "    bic_table = pd.DataFrame(rows)\n",
    "    labels = best.predict(Z)\n",
    "    probs = best.predict_proba(Z).max(axis=1)\n",
    "    df_map = pd.DataFrame({\"well_name\": wells_sub, \"x\": Z[:,0], \"y\": Z[:,1], \"cluster\": labels, \"prob\": probs})\n",
    "    sil = float(\"nan\")\n",
    "    mask = labels >= 0\n",
    "    if len(np.unique(labels[mask])) > 1:\n",
    "        try:\n",
    "            sil = float(silhouette_score(Z[mask], labels[mask], metric=\"euclidean\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return {\n",
    "        \"labels\": labels,\n",
    "        \"probabilities\": probs,\n",
    "        \"model\": best,\n",
    "        \"best_k\": int(np.unique(labels).size),\n",
    "        \"df_map\": df_map,\n",
    "        \"silhouette\": sil,\n",
    "        \"bic_table\": bic_table,\n",
    "    }\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# ------------------- АНОМАЛИИ -------------------------\n",
    "# ======================================================\n",
    "\n",
    "def lof_anomaly_scores(Z: np.ndarray, n_neighbors: int = 30) -> np.ndarray:\n",
    "    \"\"\"Возвращает аномальность в [0..1], где 1 ~ сильная аномалия.\n",
    "    На основе LocalOutlierFactor (scikit-learn), преобразуя negative_outlier_factor_.\n",
    "    \"\"\"\n",
    "    lof = LocalOutlierFactor(n_neighbors=min(n_neighbors, len(Z) - 1), novelty=False)\n",
    "    lof.fit(Z)\n",
    "    # Чем более отрицателен negative_outlier_factor_, тем более аномальна точка.\n",
    "    s = -lof.negative_outlier_factor_\n",
    "    s = (s - np.min(s)) / (np.ptp(s) + 1e-12)\n",
    "    return s\n",
    "\n",
    "\n",
    "def distance_to_medoid(Z: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Расстояние точки до медоида своего кластера в координатах embedding.\n",
    "    Для шума (-1) — расстояние до ближайшего кластера.\n",
    "    \"\"\"\n",
    "    uniq = [c for c in np.unique(labels) if c >= 0]\n",
    "    if not len(uniq):\n",
    "        return np.full(len(labels), np.nan)\n",
    "    medoids = {}\n",
    "    for c in uniq:\n",
    "        idx = np.where(labels == c)[0]\n",
    "        if idx.size == 0:\n",
    "            continue\n",
    "        D = pairwise_distances(Z[idx], metric=\"euclidean\")\n",
    "        m = idx[np.argmin(D.sum(axis=0))]\n",
    "        medoids[c] = Z[m]\n",
    "    out = np.zeros(len(labels))\n",
    "    for i, lab in enumerate(labels):\n",
    "        if lab >= 0:\n",
    "            mu = medoids.get(lab)\n",
    "            out[i] = float(np.linalg.norm(Z[i] - mu))\n",
    "        else:\n",
    "            # для шума — ближайший медоид\n",
    "            dmin = np.min([np.linalg.norm(Z[i] - mu) for mu in medoids.values()])\n",
    "            out[i] = float(dmin)\n",
    "    # нормализация [0..1]\n",
    "    out = (out - np.min(out)) / (np.ptp(out) + 1e-12)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# --------------- ПРОТОТИПЫ КЛАСТЕРОВ ------------------\n",
    "# ======================================================\n",
    "\n",
    "def _collect_matrix(panel_long: pd.DataFrame, wells: Sequence[str], channel: str, T: int) -> np.ndarray:\n",
    "    \"\"\"Собирает матрицу [n_series, T] по указанному каналу с NaN.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for w in wells:\n",
    "        g = panel_long.loc[panel_long[\"well_name\"] == w, [\"t\", channel]].sort_values(\"t\")\n",
    "        v = np.full(T, np.nan, float)\n",
    "        t = g[\"t\"].to_numpy(int)\n",
    "        vals = g[channel].to_numpy(float)\n",
    "        t = t[(t >= 0) & (t < T)]\n",
    "        vals = vals[: len(t)]\n",
    "        v[t[: len(vals)]] = vals\n",
    "        rows.append(v)\n",
    "    return np.vstack(rows) if rows else np.empty((0, T))\n",
    "\n",
    "\n",
    "def _barycenter_or_median(M: np.ndarray, method: str = \"auto\", gamma: float = 1.0, max_iter: int = 50) -> np.ndarray:\n",
    "    \"\"\"Возвращает барицентр (soft-DTW/DBA) или медиану по времени, если tslearn недоступен.\n",
    "    NaN заполняются нулями перед барицентром (так как tslearn не поддерживает NaN).\n",
    "    \"\"\"\n",
    "    if M.size == 0:\n",
    "        return M\n",
    "    if method == \"auto\":\n",
    "        method = \"softdtw\" if _TSLEARN_OK else \"median\"\n",
    "    if method in (\"softdtw\", \"dba\") and not _TSLEARN_OK:\n",
    "        method = \"median\"\n",
    "\n",
    "    if method == \"median\":\n",
    "        return np.nanmedian(M, axis=0)\n",
    "    M_filled = np.nan_to_num(M, nan=0.0)\n",
    "    if method == \"softdtw\":\n",
    "        try:\n",
    "            return softdtw_barycenter(M_filled, gamma=gamma, max_iter=max_iter)\n",
    "        except Exception:\n",
    "            return np.nanmedian(M, axis=0)\n",
    "    if method == \"dba\":\n",
    "        try:\n",
    "            return dtw_barycenter_averaging(M_filled, max_iter=max_iter)\n",
    "        except Exception:\n",
    "            return np.nanmedian(M, axis=0)\n",
    "    # fallback\n",
    "    return np.nanmedian(M, axis=0)\n",
    "\n",
    "\n",
    "def build_cluster_prototypes(\n",
    "    panel_long: pd.DataFrame,\n",
    "    df_map: pd.DataFrame,\n",
    "    channels: Sequence[str] = (\"r_oil_s\", \"wc\", \"gor\", \"r_oil_norm\"),\n",
    "    T: int = 36,\n",
    "    method: str = \"auto\",   # 'auto'|'softdtw'|'dba'|'median'\n",
    "    gamma: float = 1.0,\n",
    "    max_iter: int = 50,\n",
    ") -> Dict[int, Dict[str, np.ndarray]]:\n",
    "    \"\"\"Строит прототипы (по-канально) для каждого кластера (cluster>=0).\n",
    "    Возвращает: {cluster_id: {channel: proto_vec_length_T}}\n",
    "    Также можно использовать для отрисовки IQR/квантили — верните сырой M при необходимости.\n",
    "    \"\"\"\n",
    "    # Скважины по кластерам\n",
    "    cl2wells: Dict[int, List[str]] = {}\n",
    "    for row in df_map.itertuples(index=False):\n",
    "        if row.cluster >= 0:\n",
    "            cl2wells.setdefault(int(row.cluster), []).append(str(row.well_name))\n",
    "\n",
    "    res: Dict[int, Dict[str, np.ndarray]] = {}\n",
    "    for cl, wells in cl2wells.items():\n",
    "        res[cl] = {}\n",
    "        for ch in channels:\n",
    "            M = _collect_matrix(panel_long, wells, ch, T)\n",
    "            proto = _barycenter_or_median(M, method=method, gamma=gamma, max_iter=max_iter)\n",
    "            res[cl][ch] = proto\n",
    "    return res\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# -------------- УТИЛИТЫ ДЛЯ ОТЧЁТА --------------------\n",
    "# ======================================================\n",
    "\n",
    "def summarize_clusters(df_map: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Возвращает сводную таблицу: размер кластера, медиана prob, доля шума.\"\"\"\n",
    "    total = len(df_map)\n",
    "    noise_share = (df_map[\"cluster\"].eq(-1)).mean() if total else np.nan\n",
    "    rows = []\n",
    "    for cl, g in df_map.groupby(\"cluster\"):\n",
    "        rows.append({\n",
    "            \"cluster\": int(cl),\n",
    "            \"size\": int(len(g)),\n",
    "            \"share\": float(len(g) / total) if total else np.nan,\n",
    "            \"prob_median\": float(np.median(g[\"prob\"].values)) if len(g) else np.nan,\n",
    "        })\n",
    "    out = pd.DataFrame(rows).sort_values([\"cluster\"]).reset_index(drop=True)\n",
    "    out.attrs[\"noise_share\"] = float(noise_share)\n",
    "    return out\n",
    "\n",
    "\n",
    "def assign_anomaly_scores(df_map: pd.DataFrame, Z: np.ndarray, labels: np.ndarray, lof_k: int = 30) -> pd.DataFrame:\n",
    "    \"\"\"Добавляет к df_map столбцы: lof_score [0..1], dist_medoid [0..1], anomaly_score (среднее двух).\"\"\"\n",
    "    lof = lof_anomaly_scores(Z, n_neighbors=lof_k)\n",
    "    dmed = distance_to_medoid(Z, labels)\n",
    "    anom = 0.5 * (lof + dmed)\n",
    "    out = df_map.copy()\n",
    "    out[\"lof_score\"] = lof\n",
    "    out[\"dist_medoid\"] = dmed\n",
    "    out[\"anomaly_score\"] = anom\n",
    "    return out\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# -------------- ПРИМЕР ПОСЛЕДОВАТЕЛЬНОСТИ -------------\n",
    "# ======================================================\n",
    "\n",
    "if 'out' in globals() and 'Z_dtw' in globals() and 'sub_idx' in globals():\n",
    "    panel_long = out[\"panel_long\"]\n",
    "    wells_all = out[\"wells_used\"]\n",
    "    T = out[\"config\"].get(\"T\", 36)\n",
    "    wells_sub = np.array(wells_all)[sub_idx].tolist()\n",
    "\n",
    "    # 1) HDBSCAN\n",
    "    cfg = ClusterConfig(min_cluster_size=50, min_samples=12)\n",
    "    res = cluster_hdbscan(Z_dtw, wells_sub, cfg)\n",
    "    df_map = res[\"df_map\"]\n",
    "    print(\"Silhouette:\", res[\"silhouette\"], \" DBCV:\", res[\"dbcv\"])\n",
    "    print(summarize_clusters(df_map))\n",
    "\n",
    "    # 2) Аномалии\n",
    "    df_map = assign_anomaly_scores(df_map, Z_dtw, res[\"labels\"], lof_k=30)\n",
    "    # пример отбора аномалий\n",
    "    anomalies = df_map.sort_values(\"anomaly_score\", ascending=False).head(30)\n",
    "\n",
    "    # 3) Прототипы (медианные или soft-DTW / DBA при наличии tslearn)\n",
    "    protos = build_cluster_prototypes(panel_long, df_map, channels=(\"r_oil_s\",\"wc\",\"gor\",\"r_oil_norm\"), T=T, method=\"auto\")\n",
    "\n",
    "    # Теперь df_map можно соединить с геолого-технологическими признаками и строить объяснения.\n",
    "else:\n",
    "    print(\"Для примера нужен 'out' (Шаг 1) и 'Z_dtw, sub_idx' (Шаг 3).\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette: 0.6330978870391846  DBCV: nan\n",
      "   cluster  size     share  prob_median\n",
      "0       -1   662  0.309057     0.000000\n",
      "1        0    97  0.045285     1.000000\n",
      "2        1    54  0.025210     1.000000\n",
      "3        2    85  0.039683     1.000000\n",
      "4        3    96  0.044818     1.000000\n",
      "5        4   186  0.086835     0.876985\n",
      "6        5   120  0.056022     0.942701\n",
      "7        6   338  0.157796     0.815296\n",
      "8        7   169  0.078898     0.903724\n",
      "9        8   335  0.156396     1.000000\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "c26a647188f70b67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:48:47.233812Z",
     "start_time": "2025-09-14T11:48:25.478098Z"
    }
   },
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Production Behavior Manifold (PBM) — Шаг 5: Визуализации и Отчёты\n",
    "------------------------------------------------------------------\n",
    "Зависимости (pip): numpy, pandas, matplotlib, (опц.) jinja2\n",
    "\n",
    "Правила построения графиков (важно для окружения чат-ноутбука):\n",
    "  - использовать matplotlib (не seaborn)\n",
    "  - один график на фигуру (без субплотов)\n",
    "  - не задавать явные цвета (использовать дефолтные)\n",
    "\n",
    "Входы из предыдущих шагов:\n",
    "  - out: результат preprocess_profiles (Шаг 1)\n",
    "  - Z, sub_idx: embedding и индексы скважин, вошедшие в него (Шаг 3)\n",
    "  - res: результат cluster_hdbscan или cluster_gmm_bic (Шаг 4) с полем df_map\n",
    "  - protos: прототипы (build_cluster_prototypes)\n",
    "\n",
    "Основные функции:\n",
    "  * save_pbm_map(...) — карта PBM с кластерами/аномалиями\n",
    "  * save_cluster_prototype_plots(...) — профили прототипов с IQR-заштриховкой (если доступна сырая матрица)\n",
    "  * save_cluster_distribution_plot(...) — диаграмма распределения размеров кластеров\n",
    "  * export_csv_summaries(...) — CSV-выгрузки (карта, сводка по кластерам, топ аномалий)\n",
    "  * build_html_report(...) — простой HTML-отчёт с картинками и таблицами\n",
    "\n",
    "Пример использования внизу файла (закомментирован).\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # рендер без GUI\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------- УТИЛИТЫ -------------------------\n",
    "\n",
    "def _ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def _collect_matrix(panel_long: pd.DataFrame, wells: Sequence[str], channel: str, T: int) -> np.ndarray:\n",
    "    rows = []\n",
    "    for w in wells:\n",
    "        g = panel_long.loc[panel_long[\"well_name\"] == w, [\"t\", channel]].sort_values(\"t\")\n",
    "        v = np.full(T, np.nan, float)\n",
    "        t = g[\"t\"].to_numpy(int)\n",
    "        vals = g[channel].to_numpy(float)\n",
    "        t = t[(t >= 0) & (t < T)]\n",
    "        vals = vals[: len(t)]\n",
    "        v[t[: len(vals)]] = vals\n",
    "        rows.append(v)\n",
    "    return np.vstack(rows) if rows else np.empty((0, T))\n",
    "\n",
    "\n",
    "# ----------------------- КАРТА PBM ------------------------\n",
    "\n",
    "def save_pbm_map(\n",
    "    Z: np.ndarray,\n",
    "    df_map: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    title: str = \"PBM map (UMAP)\",\n",
    "    annotate_medoids: bool = True,\n",
    "    mark_anomalies: bool = True,\n",
    "    anomaly_col: str = \"anomaly_score\",\n",
    "    dpi: int = 160,\n",
    ") -> str:\n",
    "    \"\"\"Сохраняет PNG с картой PBM (scatter), один график — одна фигура.\n",
    "    Возвращает путь к PNG.\n",
    "    \"\"\"\n",
    "    _ensure_dir(out_dir)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.set_title(title)\n",
    "    # точки\n",
    "    sc = ax.scatter(Z[:, 0], Z[:, 1], s=12)\n",
    "    ax.set_xlabel(\"UMAP-1\")\n",
    "    ax.set_ylabel(\"UMAP-2\")\n",
    "\n",
    "    # Медоиды кластеров (на координатах Z)\n",
    "    if annotate_medoids and \"cluster\" in df_map.columns:\n",
    "        from sklearn.metrics import pairwise_distances\n",
    "        for cl, g in df_map[df_map[\"cluster\"] >= 0].groupby(\"cluster\"):\n",
    "            idx = g.index.to_numpy()\n",
    "            D = pairwise_distances(Z[idx], metric=\"euclidean\")\n",
    "            m_local = int(idx[np.argmin(D.sum(axis=0))])\n",
    "            ax.text(Z[m_local, 0], Z[m_local, 1], f\"C{int(cl)}\", fontsize=9)\n",
    "\n",
    "    # Аномалии (обводка поверх, если есть колонка)\n",
    "    if mark_anomalies and anomaly_col in df_map.columns:\n",
    "        q = df_map[anomaly_col].quantile(0.95)\n",
    "        mask = df_map[anomaly_col] >= q\n",
    "        ax.scatter(Z[mask, 0], Z[mask, 1], s=32, facecolors='none', edgecolors='k', linewidths=0.8)\n",
    "\n",
    "    path = os.path.join(out_dir, \"pbm_map.png\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, dpi=dpi)\n",
    "    plt.close(fig)\n",
    "    return path\n",
    "\n",
    "\n",
    "# --------------- РАСПРЕДЕЛЕНИЕ КЛАСТЕРОВ ----------------\n",
    "\n",
    "def save_cluster_distribution_plot(df_map: pd.DataFrame, out_dir: str, dpi: int = 160) -> str:\n",
    "    _ensure_dir(out_dir)\n",
    "    sizes = df_map[df_map[\"cluster\"] >= 0][\"cluster\"].value_counts().sort_index()\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.set_title(\"Cluster sizes\")\n",
    "    ax.bar(sizes.index.astype(str), sizes.values)\n",
    "    ax.set_xlabel(\"Cluster\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    path = os.path.join(out_dir, \"cluster_sizes.png\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, dpi=dpi)\n",
    "    plt.close(fig)\n",
    "    return path\n",
    "\n",
    "\n",
    "# ------------------ ПРОФИЛИ ПРОТОТИПОВ -------------------\n",
    "\n",
    "def save_cluster_prototype_plots(\n",
    "    panel_long: pd.DataFrame,\n",
    "    df_map: pd.DataFrame,\n",
    "    protos: Dict[int, Dict[str, np.ndarray]],\n",
    "    channels: Sequence[str],\n",
    "    T: int,\n",
    "    out_dir: str,\n",
    "    dpi: int = 160,\n",
    ") -> List[str]:\n",
    "    \"\"\"Для каждого кластера и канала рисует один график с прототипом и IQR-зоной.\n",
    "    Возвращает список путей к PNG.\n",
    "    \"\"\"\n",
    "    _ensure_dir(out_dir)\n",
    "    paths: List[str] = []\n",
    "    # подготовка списков скважин по кластеру\n",
    "    cl2wells: Dict[int, List[str]] = {}\n",
    "    for row in df_map.itertuples(index=False):\n",
    "        if row.cluster >= 0:\n",
    "            cl2wells.setdefault(int(row.cluster), []).append(str(row.well_name))\n",
    "\n",
    "    for cl, ch_dict in protos.items():\n",
    "        wells = cl2wells.get(cl, [])\n",
    "        for ch in channels:\n",
    "            proto = ch_dict.get(ch)\n",
    "            if proto is None:\n",
    "                continue\n",
    "            # Матрица ряда для IQR\n",
    "            M = _collect_matrix(panel_long, wells, ch, T)\n",
    "            p25 = np.nanpercentile(M, 25, axis=0) if M.size else None\n",
    "            p75 = np.nanpercentile(M, 75, axis=0) if M.size else None\n",
    "            x = np.arange(len(proto))\n",
    "            fig, ax = plt.subplots(figsize=(8, 4))\n",
    "            ax.set_title(f\"Cluster {cl} — {ch}\")\n",
    "            ax.plot(x, proto)\n",
    "            if M.size:\n",
    "                ax.fill_between(x, p25, p75, alpha=0.25)\n",
    "            ax.set_xlabel(\"t (months since start)\")\n",
    "            ax.set_ylabel(ch)\n",
    "            path = os.path.join(out_dir, f\"cluster_{cl}_{ch}.png\")\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(path, dpi=dpi)\n",
    "            plt.close(fig)\n",
    "            paths.append(path)\n",
    "    return paths\n",
    "\n",
    "\n",
    "# ----------------------- CSV-ЭКСПОРТ ---------------------\n",
    "\n",
    "def export_csv_summaries(\n",
    "    df_map: pd.DataFrame,\n",
    "    summary: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    top_anoms: int = 50,\n",
    ") -> Dict[str, str]:\n",
    "    _ensure_dir(out_dir)\n",
    "    paths = {}\n",
    "    p_map = os.path.join(out_dir, \"pbm_map_points.csv\")\n",
    "    df_map.to_csv(p_map, index=False)\n",
    "    paths[\"map_csv\"] = p_map\n",
    "\n",
    "    p_sum = os.path.join(out_dir, \"cluster_summary.csv\")\n",
    "    summary.to_csv(p_sum, index=False)\n",
    "    paths[\"summary_csv\"] = p_sum\n",
    "\n",
    "    if \"anomaly_score\" in df_map.columns:\n",
    "        p_an = os.path.join(out_dir, \"top_anomalies.csv\")\n",
    "        df_map.sort_values(\"anomaly_score\", ascending=False).head(top_anoms).to_csv(p_an, index=False)\n",
    "        paths[\"anomalies_csv\"] = p_an\n",
    "    return paths\n",
    "\n",
    "\n",
    "# --------------------- HTML-ОТЧЁТ -----------------------\n",
    "\n",
    "def build_html_report(\n",
    "    out_dir: str,\n",
    "    map_png: str,\n",
    "    sizes_png: str,\n",
    "    proto_pngs: Sequence[str],\n",
    "    df_map: pd.DataFrame,\n",
    "    summary: pd.DataFrame,\n",
    "    title: str = \"PBM Report\",\n",
    "    filename: str = \"PBM_report.html\",\n",
    ") -> str:\n",
    "    \"\"\"Простой статический HTML-отчёт без внешних шаблонизаторов.\n",
    "    Картинки и таблицы вставляются как <img> и <table>.\n",
    "    \"\"\"\n",
    "    _ensure_dir(out_dir)\n",
    "    def df_to_html(df: pd.DataFrame) -> str:\n",
    "        # Минимальная чистка HTML\n",
    "        return df.to_html(index=False, escape=True)\n",
    "\n",
    "    html = [\n",
    "        \"<!DOCTYPE html>\",\n",
    "        \"<html><head><meta charset='utf-8'><title>{}</title>\".format(title),\n",
    "        \"<style>body{font-family:Arial, sans-serif; margin:24px;} h2{margin-top:28px;} img{max-width:100%; height:auto;} table{border-collapse:collapse;} td,th{border:1px solid #ddd; padding:6px;}</style>\",\n",
    "        \"</head><body>\",\n",
    "        f\"<h1>{title}</h1>\",\n",
    "        \"<h2>PBM Map</h2>\",\n",
    "        f\"<img src='{os.path.relpath(map_png, out_dir)}' alt='PBM map'>\",\n",
    "        \"<h2>Cluster Sizes</h2>\",\n",
    "        f\"<img src='{os.path.relpath(sizes_png, out_dir)}' alt='Cluster sizes'>\",\n",
    "        \"<h2>Cluster Summary</h2>\",\n",
    "        df_to_html(summary),\n",
    "        \"<h2>Top Anomalies</h2>\",\n",
    "        df_to_html(df_map.sort_values('anomaly_score', ascending=False).head(50) if 'anomaly_score' in df_map.columns else df_map.head(50)),\n",
    "        \"<h2>Cluster Prototypes</h2>\",\n",
    "    ]\n",
    "    # галерея прототипов\n",
    "    for p in proto_pngs:\n",
    "        html.append(f\"<img src='{os.path.relpath(p, out_dir)}' alt='{os.path.basename(p)}'>\")\n",
    "    html.append(\"</body></html>\")\n",
    "\n",
    "    out_path = os.path.join(out_dir, filename)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(html))\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# -------------------- ПРИМЕР ИСПОЛЬЗОВАНИЯ --------------------\n",
    "# (раскомментируйте и выполните после Шагов 1–4)\n",
    "#\n",
    "# if 'out' in globals() and 'Z_dtw' in globals() and 'sub_idx' in globals() and 'res' in globals() and 'protos' in globals():\n",
    "#     panel_long = out[\"panel_long\"]\n",
    "#     T = out[\"config\"].get(\"T\", 36)\n",
    "#     df_map = res[\"df_map\"]\n",
    "#     out_dir = \"./pbm_report_exports\"  # или '/mnt/data/pbm_report_exports' в окружении чата\n",
    "# \n",
    "#     # 1) Основные картинки\n",
    "#     map_png = save_pbm_map(Z_dtw, df_map, out_dir)\n",
    "#     sizes_png = save_cluster_distribution_plot(df_map, out_dir)\n",
    "# \n",
    "#     # 2) Прототипы (oil_s, wc, gor, r_oil_norm)\n",
    "#     channels = (\"r_oil_s\",\"wc\",\"gor\",\"r_oil_norm\")\n",
    "#     proto_pngs = save_cluster_prototype_plots(panel_long, df_map, protos, channels, T, out_dir)\n",
    "# \n",
    "#     # 3) Таблицы CSV\n",
    "#     summary = summarize_clusters(df_map)\n",
    "#     csv_paths = export_csv_summaries(df_map, summary, out_dir, top_anoms=50)\n",
    "# \n",
    "#     # 4) HTML отчёт\n",
    "#     report_path = build_html_report(out_dir, map_png, sizes_png, proto_pngs, df_map, summary, title=\"PBM Report\")\n",
    "#     print(\"Готов отчёт:\", report_path)\n",
    "# else:\n",
    "#     print(\"Для примера нужен 'out', 'Z_dtw','sub_idx','res','protos'.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Готов отчёт: ./pbm_report_exports/PBM_report.html\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "265920991b7a7c64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:49:09.063663Z",
     "start_time": "2025-09-14T11:48:47.296208Z"
    }
   },
   "source": [
    "panel_long = out[\"panel_long\"]\n",
    "T = out[\"config\"][\"T\"]\n",
    "df_map = res[\"df_map\"]  # из HDBSCAN/GMM\n",
    "out_dir = \"./pbm_report_exports\"\n",
    "\n",
    "map_png   = save_pbm_map(Z_dtw, df_map, out_dir)              # карта PBM\n",
    "sizes_png = save_cluster_distribution_plot(df_map, out_dir)   # размеры кластеров\n",
    "proto_pngs = save_cluster_prototype_plots(\n",
    "    panel_long, df_map, protos, channels=(\"r_oil_s\",\"wc\",\"gor\",\"r_oil_norm\"), T=T, out_dir=out_dir\n",
    ")\n",
    "\n",
    "summary = summarize_clusters(df_map)\n",
    "csvs = export_csv_summaries(df_map, summary, out_dir, top_anoms=50)\n",
    "\n",
    "report = build_html_report(out_dir, map_png, sizes_png, proto_pngs, df_map, summary, title=\"PBM Report\")\n",
    "print(\"Отчёт:\", report)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отчёт: ./pbm_report_exports/PBM_report.html\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "161158aa9e2144d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e431ca2b",
   "metadata": {},
   "source": [
    "\n",
    "# Шаг 6. Прогноз профиля по префиксу (20 → 100)\n",
    "\n",
    "**Цель:** прогнозировать месяцы 21–100 для каждой скважины, используя только первые 20 месяцев и без утечки информации.\n",
    "\n",
    "Подходы в этом шаге:\n",
    "1. **KNN-достройка по префиксу** с амплитудным выравниванием соседей (по МНК на префиксе).\n",
    "2. **Мультивыходная ElasticNet-регрессия** на компактных признаках префикса.\n",
    "\n",
    "Метрики: RMSE и sMAPE на окне 21–100.  \n",
    "Отчёты и артефакты сохраняются в `./forecast_exports`.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:54:11.050816Z",
     "start_time": "2025-09-14T11:54:11.042683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\"\"\"\n",
    "forecast_addon.py — Utilities for prefix→suffix forecasting on well production profiles.\n",
    "\n",
    "Assumptions:\n",
    "- You have a long-format DataFrame `panel_long` with columns: well_name, t (0..T-1), r_oil_s, wc, gor.\n",
    "- The preprocessing aligned profiles so t=0 is production start (already done in your PBM Step 1).\n",
    "- `out` is a dict from your pipeline with keys: \"panel_long\", \"wells_used\", \"config\" with {\"T\": int}.\n",
    "\n",
    "This module provides:\n",
    "- build_prefix_scaled_channel(): scale r_oil_s by prefix-quantile for leakage-free comparison\n",
    "- make_matrices(): construct prefix matrix X_pref and full target matrix Y_full\n",
    "- knn_forecast(): neighbor-based completion with per-neighbor amplitude alignment\n",
    "- multioutput_forecast(): elastic-net multi-output baseline on compact prefix features\n",
    "- evaluate_forecasts(): RMSE and sMAPE metrics\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "def build_prefix_scaled_channel(panel_long: pd.DataFrame, wells: list, T:int, T_pref:int,\n",
    "                                q:float=0.90, eps:float=1e-9, clip_max:float=3.0,\n",
    "                                rate_col:str=\"r_oil_s\", out_col:str=\"r_oil_pref_norm\") -> pd.DataFrame:\n",
    "    \"\"\"Create leakage-free, prefix-scaled oil-rate channel (normalized by prefix q-quantile).\"\"\"\n",
    "    pl = panel_long.copy()\n",
    "    pl = pl[pl[\"t\"].between(0, T-1)].copy()\n",
    "    # compute prefix scale per well\n",
    "    scales = (\n",
    "        pl[pl[\"t\"] < T_pref]\n",
    "        .groupby(\"well_name\")[rate_col]\n",
    "        .quantile(q)\n",
    "        .rename(\"scale\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    pl = pl.merge(scales, on=\"well_name\", how=\"left\")\n",
    "    pl[out_col] = (pl[rate_col] / (pl[\"scale\"].abs() + eps)).clip(lower=0, upper=clip_max)\n",
    "    return pl.drop(columns=[\"scale\"])\n",
    "\n",
    "def make_matrices(panel_long: pd.DataFrame, wells: list, T:int, T_pref:int,\n",
    "                  channel:str=\"r_oil_pref_norm\", target_col:str=\"r_oil_s\") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return X_pref [N, T_pref], Y_suffix [N, T-T_pref], Y_full [N, T] for the given channels.\"\"\"\n",
    "    idx = {w:i for i,w in enumerate(wells)}\n",
    "    N = len(wells)\n",
    "    X = np.full((N, T_pref), np.nan)\n",
    "    Yfull = np.full((N, T), np.nan)\n",
    "    for w, g in panel_long.groupby(\"well_name\", sort=False):\n",
    "        i = idx.get(w, None)\n",
    "        if i is None: continue\n",
    "        gg = g.sort_values(\"t\")\n",
    "        t = gg[\"t\"].to_numpy()\n",
    "        if len(t)==0: continue\n",
    "        # fill prefix channel\n",
    "        v = gg[channel].to_numpy()\n",
    "        mask_pref = (t >= 0) & (t < T_pref)\n",
    "        if mask_pref.any():\n",
    "            X[i, t[mask_pref]] = v[mask_pref]\n",
    "        # fill full target\n",
    "        vy = gg[target_col].to_numpy()\n",
    "        mask_full = (t >= 0) & (t < T)\n",
    "        if mask_full.any():\n",
    "            Yfull[i, t[mask_full]] = vy[mask_full]\n",
    "    Ysuffix = Yfull[:, T_pref:T]\n",
    "    return X, Ysuffix, Yfull\n",
    "\n",
    "def _align_scale(y_ref_pref: np.ndarray, y_nei_pref: np.ndarray, eps:float=1e-9) -> float:\n",
    "    \"\"\"Least-squares scale factor to match neighbor prefix to reference prefix.\"\"\"\n",
    "    num = np.nansum(y_ref_pref * y_nei_pref)\n",
    "    den = np.nansum(y_nei_pref ** 2) + eps\n",
    "    s = num / den\n",
    "    if not np.isfinite(s):\n",
    "        s = 1.0\n",
    "    return float(s)\n",
    "\n",
    "def knn_forecast(X_pref: np.ndarray, Y_full: np.ndarray, T_pref:int, K:int=15) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
    "    \"\"\"KNN completion with per-neighbor amplitude alignment on prefix. Returns pred_suffix [N, T-T_pref].\"\"\"\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    N, T_pref_ = X_pref.shape\n",
    "    assert T_pref_ == T_pref\n",
    "    # Use only wells with complete full horizon for training & evaluation\n",
    "    mask_full = np.isfinite(Y_full).sum(axis=1) >= Y_full.shape[1]\n",
    "    I = np.where(mask_full)[0]\n",
    "    if len(I) < 3:\n",
    "        raise ValueError(\"Not enough wells with full horizon for KNN.\")\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(K+1, len(I)), metric=\"euclidean\").fit(X_pref[I])\n",
    "    dists, knn_idx = nbrs.kneighbors(X_pref[I])\n",
    "    T_suffix = Y_full.shape[1] - T_pref\n",
    "    pred = np.zeros((len(I), T_suffix))\n",
    "    used = []\n",
    "    for r, (row_knn, row_d) in enumerate(zip(knn_idx, dists)):\n",
    "        # exclude self (distance ~ 0)\n",
    "        neigh = [j for j,d in zip(row_knn, row_d) if d>0][:K]\n",
    "        if not neigh:\n",
    "            # fallback: take the single closest even if self\n",
    "            neigh = [row_knn[0]]\n",
    "        # gather neighbors (global indices)\n",
    "        neigh_g = I[neigh]\n",
    "        # align each neighbor by LS scale over prefix (on raw target y since it's in original units)\n",
    "        y_ref_pref = Y_full[I[r], :T_pref]\n",
    "        suffixes = []\n",
    "        for g in neigh_g:\n",
    "            y_nei_pref = Y_full[g, :T_pref]\n",
    "            s = _align_scale(y_ref_pref, y_nei_pref)\n",
    "            y_nei_suffix = s * Y_full[g, T_pref:]\n",
    "            suffixes.append(y_nei_suffix)\n",
    "        suffixes = np.vstack(suffixes)\n",
    "        pred[r] = np.nanmedian(suffixes, axis=0)\n",
    "        used.append(neigh_g)\n",
    "    # Build a full-N predictions array filled with nan, then place known rows\n",
    "    Ypred_fullN = np.full((N, T_suffix), np.nan)\n",
    "    Ypred_fullN[I] = pred\n",
    "    return Ypred_fullN, {\"train_indices\": I, \"neighbors\": used}\n",
    "\n",
    "def _safe_import_build_side_features():\n",
    "    try:\n",
    "        # Inherit from the user's notebook if defined\n",
    "        from build_side_features import build_side_features  # unlikely\n",
    "        return build_side_features\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _fallback_prefix_features(panel_long: pd.DataFrame, wells:list, T_pref:int) -> pd.DataFrame:\n",
    "    \"\"\"Compact features from prefix window: mean, std, slope, curvature, wc stats.\"\"\"\n",
    "    rows = []\n",
    "    for w, g in panel_long[panel_long[\"t\"].between(0, T_pref-1)].groupby(\"well_name\", sort=False):\n",
    "        gg = g.sort_values(\"t\")\n",
    "        t = gg[\"t\"].to_numpy().astype(float)\n",
    "        y = gg[\"r_oil_pref_norm\"].to_numpy()\n",
    "        wc = gg.get(\"wc\", pd.Series([np.nan]*len(gg))).to_numpy()\n",
    "        # basic stats\n",
    "        mu, sd = np.nanmean(y), np.nanstd(y)\n",
    "        # slope via linear fit\n",
    "        if len(t) >= 2 and np.nanstd(t) > 0:\n",
    "            A = np.vstack([t, np.ones_like(t)]).T\n",
    "            m, b = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "        else:\n",
    "            m, b = 0.0, float(y[0]) if len(y) else 0.0\n",
    "        # curvature (2nd diff)\n",
    "        if len(y) >= 3:\n",
    "            curv = np.nanmean(np.diff(y,2))\n",
    "        else:\n",
    "            curv = 0.0\n",
    "        rows.append({\n",
    "            \"well_name\": w,\n",
    "            \"y_mean\": mu,\n",
    "            \"y_std\": sd,\n",
    "            \"y_slope\": m,\n",
    "            \"y_intercept\": b,\n",
    "            \"y_curv\": curv,\n",
    "            \"wc_mean\": float(np.nanmean(wc)),\n",
    "            \"wc_std\": float(np.nanstd(wc)),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def multioutput_forecast(panel_long: pd.DataFrame, wells:list, T:int, T_pref:int,\n",
    "                         Y_full: np.ndarray, random_state:int=43) -> Tuple[np.ndarray, Dict]:\n",
    "    \"\"\"ElasticNetCV-based multioutput regression on compact prefix features.\"\"\"\n",
    "    from sklearn.linear_model import ElasticNetCV\n",
    "    from sklearn.multioutput import MultiOutputRegressor\n",
    "    # Build compact features\n",
    "    feats = _fallback_prefix_features(panel_long, wells, T_pref)\n",
    "    # align feature rows with wells order\n",
    "    idx = {w:i for i,w in enumerate(wells)}\n",
    "    X = np.full((len(wells), feats.shape[1]-1), np.nan)\n",
    "    for _, r in feats.iterrows():\n",
    "        i = idx[r[\"well_name\"]]\n",
    "        X[i] = r.drop(labels=[\"well_name\"]).to_numpy(dtype=float)\n",
    "    # training mask (full horizon)\n",
    "    mask_full = np.isfinite(Y_full).sum(axis=1) >= T\n",
    "    I = np.where(mask_full)[0]\n",
    "    if len(I) < 3:\n",
    "        raise ValueError(\"Not enough wells with full horizon for MultiOutput.\")\n",
    "    Y = Y_full[I, T_pref:T]\n",
    "    model = MultiOutputRegressor(ElasticNetCV(l1_ratio=[0.1,0.5,0.9], cv=5, random_state=random_state))\n",
    "    model.fit(X[I], Y)\n",
    "    Ypred = np.full((len(wells), T-T_pref), np.nan)\n",
    "    Ypred[I] = model.predict(X[I])\n",
    "    return Ypred, {\"train_indices\": I, \"model\": model, \"features\": feats}\n",
    "\n",
    "def evaluate_forecasts(Y_true, Y_pred):\n",
    "    \"\"\"RMSE и sMAPE по строкам, где и факт, и прогноз без NaN; без зависимости от 'squared'.\"\"\"\n",
    "    import numpy as np\n",
    "    mask = np.isfinite(Y_pred).all(axis=1) & np.isfinite(Y_true).all(axis=1)\n",
    "    n_eval = int(mask.sum())\n",
    "    if n_eval == 0:\n",
    "        return {\"rmse\": float(\"nan\"), \"smape\": float(\"nan\"), \"n_eval\": 0}\n",
    "    diff = Y_pred[mask] - Y_true[mask]\n",
    "    rmse = float(np.sqrt(np.mean(diff**2)))\n",
    "    smape = float(np.nanmean(2*np.abs(Y_pred[mask]-Y_true[mask])/(np.abs(Y_pred[mask])+np.abs(Y_true[mask])+1e-9)))\n",
    "    return {\"rmse\": rmse, \"smape\": smape, \"n_eval\": n_eval}"
   ],
   "id": "7006e05bf9c28f17",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "5004e44c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:54:18.360620Z",
     "start_time": "2025-09-14T11:54:15.070164Z"
    }
   },
   "source": [
    "# === Шаг 6: Прогноз профиля по префиксу (20 → 100) ===\n",
    "import os, numpy as np, pandas as pd, json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert 'out' in globals(), \"Требуется объект 'out' из Шага 1 (preprocess_profiles).\"\n",
    "panel_long = out[\"panel_long\"].copy()\n",
    "wells_used = out[\"wells_used\"]\n",
    "T = int(out[\"config\"][\"T\"])\n",
    "T_pref = 20  # можно вынести в конфиг\n",
    "\n",
    "# 6.1. Построить префикс-нормированный канал без утечки\n",
    "panel_long = build_prefix_scaled_channel(panel_long, wells_used, T=T, T_pref=T_pref,\n",
    "                                         q=0.90, rate_col=\"r_oil_s\", out_col=\"r_oil_pref_norm\")\n",
    "\n",
    "# 6.2. Матрицы X_pref, Y_suffix, Y_full\n",
    "X_pref, Y_suffix_true, Y_full = make_matrices(panel_long, wells_used, T=T, T_pref=T_pref,\n",
    "                                              channel=\"r_oil_pref_norm\", target_col=\"r_oil_s\")\n",
    "\n",
    "# 6.3. KNN-достройка\n",
    "Y_pred_knn, knn_info = knn_forecast(X_pref, Y_full, T_pref=T_pref, K=15)\n",
    "\n",
    "# 6.4. Мультивыходная регрессия\n",
    "Y_pred_lr, lr_info = multioutput_forecast(panel_long, wells_used, T=T, T_pref=T_pref, Y_full=Y_full, random_state=43)\n",
    "\n",
    "# 6.5. Оценка качества\n",
    "m_knn = evaluate_forecasts(Y_suffix_true, Y_pred_knn)\n",
    "m_lr  = evaluate_forecasts(Y_suffix_true, Y_pred_lr)\n",
    "print(\"KNN   → RMSE={rmse:.4f}, sMAPE={smape:.4f}, N={n_eval}\".format(**m_knn))\n",
    "print(\"ENet  → RMSE={rmse:.4f}, sMAPE={smape:.4f}, N={n_eval}\".format(**m_lr))\n",
    "\n",
    "# 6.6. Сохранение прогнозов и отчёта\n",
    "out_dir = \"./forecast_exports\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(out_dir, \"Y_suffix_true.npy\"), Y_suffix_true)\n",
    "np.save(os.path.join(out_dir, \"Y_pred_knn.npy\"), Y_pred_knn)\n",
    "np.save(os.path.join(out_dir, \"Y_pred_enet.npy\"), Y_pred_lr)\n",
    "\n",
    "# Таблица метрик\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\"model\": \"knn\", \"rmse\": m_knn[\"rmse\"], \"smape\": m_knn[\"smape\"], \"n_eval\": m_knn[\"n_eval\"]},\n",
    "    {\"model\": \"elasticnet\", \"rmse\": m_lr[\"rmse\"], \"smape\": m_lr[\"smape\"], \"n_eval\": m_lr[\"n_eval\"]},\n",
    "])\n",
    "metrics_csv = os.path.join(out_dir, \"metrics.csv\")\n",
    "metrics_df.to_csv(metrics_csv, index=False)\n",
    "\n",
    "# Пер-скважинные прогнозы (с именами)\n",
    "def save_predictions_csv(Y_pred: np.ndarray, wells: list, name:str):\n",
    "    cols = [f\"m{t}\" for t in range(T_pref+1, T+1)]\n",
    "    df = pd.DataFrame(Y_pred, columns=cols)\n",
    "    df.insert(0, \"well_name\", wells)\n",
    "    df.to_csv(os.path.join(out_dir, f\"pred_{name}.csv\"), index=False)\n",
    "\n",
    "save_predictions_csv(Y_pred_knn, wells_used, \"knn\")\n",
    "save_predictions_csv(Y_pred_lr,  wells_used, \"elasticnet\")\n",
    "\n",
    "# 6.7. Примеры графиков \"факт vs прогноз\"\n",
    "def plot_example(idx, title, ytrue, ypred):\n",
    "    plt.figure()\n",
    "    plt.plot(range(T_pref, T), ytrue[idx], label=\"true\")\n",
    "    plt.plot(range(T_pref, T), ypred[idx], label=\"pred\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"month index\")\n",
    "    plt.ylabel(\"oil rate (r_oil_s)\")\n",
    "    plt.legend()\n",
    "    fig_path = os.path.join(out_dir, f\"{title.replace(' ','_').lower()}_{idx}.png\")\n",
    "    plt.savefig(fig_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    return fig_path\n",
    "\n",
    "# случайные 3 примера из обучаемых скважин\n",
    "rng = np.random.default_rng(43)\n",
    "I = np.where(np.isfinite(Y_pred_knn).all(axis=1))[0]\n",
    "show = rng.choice(I, size=min(3, len(I)), replace=False) if len(I) else []\n",
    "example_imgs = []\n",
    "for i in show:\n",
    "    example_imgs.append(plot_example(i, \"knn_example\", Y_suffix_true, Y_pred_knn))\n",
    "    example_imgs.append(plot_example(i, \"enet_example\", Y_suffix_true, Y_pred_lr))\n",
    "\n",
    "# 6.8. Простой HTML отчёт\n",
    "html = f\"\"\"\n",
    "<html><head><meta charset='utf-8'><title>Forecast Report</title></head><body>\n",
    "<h2>Forecast evaluation (prefix {T_pref} → total {T})</h2>\n",
    "<p>Generated: {datetime.utcnow().isoformat()}Z</p>\n",
    "<table border='1' cellspacing='0' cellpadding='6'>\n",
    "<tr><th>Model</th><th>RMSE</th><th>sMAPE</th><th>N eval wells</th></tr>\n",
    "<tr><td>KNN</td><td>{m_knn['rmse']:.4f}</td><td>{m_knn['smape']:.4f}</td><td>{m_knn['n_eval']}</td></tr>\n",
    "<tr><td>ElasticNet</td><td>{m_lr['rmse']:.4f}</td><td>{m_lr['smape']:.4f}</td><td>{m_lr['n_eval']}</td></tr>\n",
    "</table>\n",
    "<h3>Files</h3>\n",
    "<ul>\n",
    "  <li>metrics.csv</li>\n",
    "  <li>pred_knn.csv</li>\n",
    "  <li>pred_elasticnet.csv</li>\n",
    "</ul>\n",
    "<h3>Examples</h3>\n",
    "{''.join(f\"<img src='{os.path.basename(p)}' style='max-width:640px;display:block;margin-bottom:10px;'/>\" for p in example_imgs)}\n",
    "</body></html>\n",
    "\"\"\"\n",
    "report_path = os.path.join(out_dir, \"forecast_report.html\")\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html)\n",
    "\n",
    "print(\"Saved:\", metrics_csv, \"and\", report_path)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN   → RMSE=20.3636, sMAPE=0.4884, N=1964\n",
      "ENet  → RMSE=36.7395, sMAPE=1.1923, N=1964\n",
      "Saved: ./forecast_exports/metrics.csv and ./forecast_exports/forecast_report.html\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:53:21.188758Z",
     "start_time": "2025-09-14T11:53:21.187291Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "969baf7a0d0b9acb",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8e3b790eee64879f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
